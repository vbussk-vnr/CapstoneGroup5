{
  "timestamp": "2026-01-24T03:37:04.438835",
  "total_experiments": 7,
  "experiments": [
    {
      "experiment": "cs_baseline_dense_minilm",
      "config": {
        "name": "cs_baseline_dense_minilm",
        "domain": "customer_support",
        "subset": "techqa",
        "description": "Baseline: Dense retrieval with MiniLM embeddings",
        "chunking_strategy": "sentence_based",
        "chunk_size": 512,
        "chunk_overlap": 50,
        "embedding_model": "sentence-transformers/all-MiniLM-L6-v2",
        "retrieval_type": "dense",
        "top_k_final": 5,
        "use_cross_encoder": false,
        "hybrid_alpha": 0.0,
        "expected_improvement": "0% (Baseline)",
        "rationale": "Standard dense retrieval baseline"
      },
      "comparison": {
        "relevance_score": {
          "rmse": 0.4687767129848032,
          "aucroc": null,
          "mean_predicted": 0.38391655280640036,
          "mean_ground_truth": 0.07031843351735237,
          "std_predicted": 0.3422912066262442,
          "std_ground_truth": 0.10387687046787056
        },
        "utilization_score": {
          "rmse": 0.2489931847059674,
          "aucroc": null,
          "mean_predicted": 0.16155560709526406,
          "mean_ground_truth": 0.02589249433268999,
          "std_predicted": 0.21514119438610838,
          "std_ground_truth": 0.04010633613234541
        },
        "completeness_score": {
          "rmse": 0.5883524625514124,
          "aucroc": null,
          "mean_predicted": 0.45238215121666103,
          "mean_ground_truth": 0.5815805254185372,
          "std_predicted": 0.38428413836006414,
          "std_ground_truth": 0.38866380494493075
        },
        "adherence_score": {
          "rmse": null,
          "aucroc": 0.6093938177438779,
          "mean_predicted": 0.35,
          "mean_ground_truth": 0.53,
          "std_predicted": 0.4769696007084728,
          "std_ground_truth": 0.49909918853871116
        }
      }
    },
    {
      "experiment": "cs_baseline_sparse_bm25",
      "config": {
        "name": "cs_baseline_sparse_bm25",
        "domain": "customer_support",
        "subset": "techqa",
        "description": "Baseline: Sparse (BM25) - captures keywords in FAQ",
        "chunking_strategy": "sentence_based",
        "chunk_size": 512,
        "chunk_overlap": 50,
        "embedding_model": "sentence-transformers/all-MiniLM-L6-v2",
        "retrieval_type": "sparse",
        "top_k_final": 5,
        "use_cross_encoder": false,
        "hybrid_alpha": 0.0,
        "expected_improvement": "0% (Keyword baseline)",
        "rationale": "BM25 good for matching FAQ keywords"
      },
      "comparison": {
        "relevance_score": {
          "rmse": 0.3908793542751022,
          "aucroc": null,
          "mean_predicted": 0.3112449075286491,
          "mean_ground_truth": 0.07031843351735237,
          "std_predicted": 0.3026591375714823,
          "std_ground_truth": 0.10387687046787056
        },
        "utilization_score": {
          "rmse": 0.18552589650099843,
          "aucroc": null,
          "mean_predicted": 0.11039183005097497,
          "mean_ground_truth": 0.02589249433268999,
          "std_predicted": 0.17438690797079093,
          "std_ground_truth": 0.04010633613234541
        },
        "completeness_score": {
          "rmse": 0.5603132483373104,
          "aucroc": null,
          "mean_predicted": 0.413800965159448,
          "mean_ground_truth": 0.5815805254185372,
          "std_predicted": 0.3803520960501511,
          "std_ground_truth": 0.38866380494493075
        },
        "adherence_score": {
          "rmse": null,
          "aucroc": 0.5527900441589723,
          "mean_predicted": 0.29,
          "mean_ground_truth": 0.53,
          "std_predicted": 0.4537620521815371,
          "std_ground_truth": 0.49909918853871116
        }
      }
    },
    {
      "experiment": "cs_exp1_hybrid_dense_sparse",
      "config": {
        "name": "cs_exp1_hybrid_dense_sparse",
        "domain": "customer_support",
        "subset": "techqa",
        "description": "Hybrid: Combine dense (semantic) + sparse (lexical)",
        "chunking_strategy": "sentence_based",
        "chunk_size": 512,
        "chunk_overlap": 50,
        "embedding_model": "sentence-transformers/all-MiniLM-L6-v2",
        "retrieval_type": "hybrid",
        "top_k_final": 5,
        "use_cross_encoder": false,
        "hybrid_alpha": 0.5,
        "expected_improvement": "+15-25%",
        "rationale": "Customer support queries have specific keywords + semantic meaning"
      },
      "comparison": {
        "relevance_score": {
          "rmse": 0.4097048390492642,
          "aucroc": null,
          "mean_predicted": 0.3593573188250175,
          "mean_ground_truth": 0.07031843351735237,
          "std_predicted": 0.28876912202858385,
          "std_ground_truth": 0.10387687046787056
        },
        "utilization_score": {
          "rmse": 0.2760739610729103,
          "aucroc": null,
          "mean_predicted": 0.1876146951826606,
          "mean_ground_truth": 0.02589249433268999,
          "std_predicted": 0.22545277008893064,
          "std_ground_truth": 0.04010633613234541
        },
        "completeness_score": {
          "rmse": 0.5922283493382884,
          "aucroc": null,
          "mean_predicted": 0.5276108099449657,
          "mean_ground_truth": 0.5815805254185372,
          "std_predicted": 0.4064617220467976,
          "std_ground_truth": 0.38866380494493075
        },
        "adherence_score": {
          "rmse": null,
          "aucroc": 0.5008028904054597,
          "mean_predicted": 0.32,
          "mean_ground_truth": 0.53,
          "std_predicted": 0.46647615158762396,
          "std_ground_truth": 0.49909918853871116
        }
      }
    },
    {
      "experiment": "cs_exp2_hybrid_rerank",
      "config": {
        "name": "cs_exp2_hybrid_rerank",
        "domain": "customer_support",
        "subset": "techqa",
        "description": "Hybrid + Cross-encoder reranking",
        "chunking_strategy": "sentence_based",
        "chunk_size": 512,
        "chunk_overlap": 50,
        "embedding_model": "sentence-transformers/all-MiniLM-L6-v2",
        "retrieval_type": "hybrid",
        "top_k_final": 5,
        "use_cross_encoder": true,
        "cross_encoder_model": "cross-encoder/ms-marco-MiniLM-L-6-v2",
        "hybrid_alpha": 0.5,
        "expected_improvement": "+25-35%",
        "rationale": "Reranking improves relevance of top results"
      },
      "comparison": {
        "relevance_score": {
          "rmse": 0.4555906722865486,
          "aucroc": null,
          "mean_predicted": 0.39712133823815526,
          "mean_ground_truth": 0.07031843351735237,
          "std_predicted": 0.3231866493817902,
          "std_ground_truth": 0.10387687046787056
        },
        "utilization_score": {
          "rmse": 0.2816386211421429,
          "aucroc": null,
          "mean_predicted": 0.18457478001495217,
          "mean_ground_truth": 0.02589249433268999,
          "std_predicted": 0.24106445933917334,
          "std_ground_truth": 0.04010633613234541
        },
        "completeness_score": {
          "rmse": 0.608910000515993,
          "aucroc": null,
          "mean_predicted": 0.39942032944860983,
          "mean_ground_truth": 0.5815805254185372,
          "std_predicted": 0.38272055818976125,
          "std_ground_truth": 0.38866380494493075
        },
        "adherence_score": {
          "rmse": null,
          "aucroc": 0.5549979927739864,
          "mean_predicted": 0.42,
          "mean_ground_truth": 0.53,
          "std_predicted": 0.49355850717012273,
          "std_ground_truth": 0.49909918853871116
        }
      }
    },
    {
      "experiment": "cs_exp3_hybrid_mpnet",
      "config": {
        "name": "cs_exp3_hybrid_mpnet",
        "domain": "customer_support",
        "subset": "techqa",
        "description": "Hybrid with stronger embeddings (MPNet-base)",
        "chunking_strategy": "sentence_based",
        "chunk_size": 512,
        "chunk_overlap": 50,
        "embedding_model": "sentence-transformers/all-mpnet-base-v2",
        "retrieval_type": "hybrid",
        "top_k_final": 5,
        "use_cross_encoder": false,
        "hybrid_alpha": 0.5,
        "expected_improvement": "+10-20%",
        "rationale": "MPNet > MiniLM for semantic understanding"
      },
      "comparison": {
        "relevance_score": {
          "rmse": 0.400769964391777,
          "aucroc": null,
          "mean_predicted": 0.37689856917946934,
          "mean_ground_truth": 0.07031843351735237,
          "std_predicted": 0.28651050461998384,
          "std_ground_truth": 0.10387687046787056
        },
        "utilization_score": {
          "rmse": 0.28641082105640286,
          "aucroc": null,
          "mean_predicted": 0.20030479308216426,
          "mean_ground_truth": 0.02589249433268999,
          "std_predicted": 0.23289968383668444,
          "std_ground_truth": 0.04010633613234541
        },
        "completeness_score": {
          "rmse": 0.6099246380139767,
          "aucroc": null,
          "mean_predicted": 0.49932584437008953,
          "mean_ground_truth": 0.5815805254185372,
          "std_predicted": 0.41501094237742997,
          "std_ground_truth": 0.38866380494493075
        },
        "adherence_score": {
          "rmse": null,
          "aucroc": 0.520875150541951,
          "mean_predicted": 0.32,
          "mean_ground_truth": 0.53,
          "std_predicted": 0.46647615158762396,
          "std_ground_truth": 0.49909918853871116
        }
      }
    },
    {
      "experiment": "cs_exp4_semantic_chunking_mpnet",
      "config": {
        "name": "cs_exp4_semantic_chunking_mpnet",
        "domain": "customer_support",
        "subset": "techqa",
        "description": "Semantic-aware chunking (groups similar sentences)",
        "chunking_strategy": "semantic",
        "chunk_size": 512,
        "chunk_overlap": 50,
        "embedding_model": "sentence-transformers/all-mpnet-base-v2",
        "retrieval_type": "hybrid",
        "top_k_final": 5,
        "use_cross_encoder": false,
        "hybrid_alpha": 0.5,
        "expected_improvement": "+20-30%",
        "rationale": "Semantic chunking preserves FAQ answer structure"
      },
      "comparison": {
        "relevance_score": {
          "rmse": 0.5054284122363504,
          "aucroc": null,
          "mean_predicted": 0.44396685760826493,
          "mean_ground_truth": 0.07031843351735237,
          "std_predicted": 0.3517824503777245,
          "std_ground_truth": 0.10387687046787056
        },
        "utilization_score": {
          "rmse": 0.30126061405764243,
          "aucroc": null,
          "mean_predicted": 0.19681886298885096,
          "mean_ground_truth": 0.02589249433268999,
          "std_predicted": 0.2485879000963029,
          "std_ground_truth": 0.04010633613234541
        },
        "completeness_score": {
          "rmse": 0.5997088826545106,
          "aucroc": null,
          "mean_predicted": 0.4047285013767978,
          "mean_ground_truth": 0.5815805254185372,
          "std_predicted": 0.3724287019038055,
          "std_ground_truth": 0.38866380494493075
        },
        "adherence_score": {
          "rmse": null,
          "aucroc": 0.6023685266961061,
          "mean_predicted": 0.3,
          "mean_ground_truth": 0.53,
          "std_predicted": 0.45825756949558394,
          "std_ground_truth": 0.49909918853871116
        }
      }
    },
    {
      "experiment": "cs_exp5_full_pipeline_optimized",
      "config": {
        "name": "cs_exp5_full_pipeline_optimized",
        "domain": "customer_support",
        "subset": "techqa",
        "description": "BEST: Semantic + MPNet + Hybrid + Reranking",
        "chunking_strategy": "semantic",
        "chunk_size": 512,
        "chunk_overlap": 50,
        "embedding_model": "sentence-transformers/all-mpnet-base-v2",
        "retrieval_type": "hybrid",
        "top_k_final": 5,
        "use_cross_encoder": true,
        "cross_encoder_model": "cross-encoder/ms-marco-MiniLM-L-6-v2",
        "hybrid_alpha": 0.5,
        "expected_improvement": "+40-50%",
        "rationale": "Best-of-all-worlds approach"
      },
      "comparison": {
        "relevance_score": {
          "rmse": 0.5110565910938983,
          "aucroc": null,
          "mean_predicted": 0.4739312840137702,
          "mean_ground_truth": 0.07031843351735237,
          "std_predicted": 0.33516080803440496,
          "std_ground_truth": 0.10387687046787056
        },
        "utilization_score": {
          "rmse": 0.3264730407511907,
          "aucroc": null,
          "mean_predicted": 0.2259162236527653,
          "mean_ground_truth": 0.02589249433268999,
          "std_predicted": 0.2637419318153466,
          "std_ground_truth": 0.04010633613234541
        },
        "completeness_score": {
          "rmse": 0.5559205138191005,
          "aucroc": null,
          "mean_predicted": 0.4585535301324276,
          "mean_ground_truth": 0.5815805254185372,
          "std_predicted": 0.38162933184546544,
          "std_ground_truth": 0.38866380494493075
        },
        "adherence_score": {
          "rmse": null,
          "aucroc": 0.5291047771979125,
          "mean_predicted": 0.35,
          "mean_ground_truth": 0.53,
          "std_predicted": 0.4769696007084728,
          "std_ground_truth": 0.49909918853871116
        }
      }
    }
  ]
}