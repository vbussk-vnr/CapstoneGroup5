import hashlib
import pickle
import json
from pathlib import Path
from typing import List, Dict, Optional, Any

class ChunkCacheManager:
    """
    Manages chunk caching to avoid re-chunking when only non-chunking parameters change.

    Cache key is based on:
    - Document content hash
    - Chunking strategy
    - Chunk size
    - Chunk overlap
    - Embedding model (for semantic chunking)
    """

    def __init__(self, cache_dir: str = "experiments/chunk_cache"):
        self.cache_dir = Path(cache_dir)
        self.cache_dir.mkdir(parents=True, exist_ok=True)

    def _compute_cache_key(self, documents: List[str], chunking_strategy: str,
                          chunk_size: int, chunk_overlap: int, embedding_model: str = None) -> str:
        """Compute a unique cache key based on input parameters."""
        # Create content hash
        content = '\n'.join(sorted(documents))  # Sort to ensure consistent ordering
        content_hash = hashlib.md5(content.encode()).hexdigest()[:12]

        # Create parameter string
        params = f"{chunking_strategy}_{chunk_size}_{chunk_overlap}"
        if embedding_model:
            params += f"_{embedding_model.replace('/', '_')}"

        return f"chunks_{content_hash}_{hashlib.md5(params.encode()).hexdigest()[:8]}"

    def get_cache_path(self, cache_key: str) -> Path:
        """Get the cache file path for a given key."""
        return self.cache_dir / f"{cache_key}.pkl"

    def get_meta_path(self, cache_key: str) -> Path:
        """Get the metadata file path for a given key."""
        return self.cache_dir / f"{cache_key}_meta.json"

    def is_cached(self, documents: List[str], chunking_strategy: str,
                  chunk_size: int, chunk_overlap: int, embedding_model: str = None) -> bool:
        """Check if chunks are cached for the given parameters."""
        cache_key = self._compute_cache_key(documents, chunking_strategy, chunk_size, chunk_overlap, embedding_model)
        cache_path = self.get_cache_path(cache_key)
        meta_path = self.get_meta_path(cache_key)

        return cache_path.exists() and meta_path.exists()

    def load_chunks(self, documents: List[str], chunking_strategy: str,
                   chunk_size: int, chunk_overlap: int, embedding_model: str = None) -> Optional[List[Dict]]:
        """Load cached chunks if available."""
        if not self.is_cached(documents, chunking_strategy, chunk_size, chunk_overlap, embedding_model):
            return None

        cache_key = self._compute_cache_key(documents, chunking_strategy, chunk_size, chunk_overlap, embedding_model)
        cache_path = self.get_cache_path(cache_key)

        try:
            with open(cache_path, 'rb') as f:
                chunks = pickle.load(f)

            # Load metadata for verification
            meta_path = self.get_meta_path(cache_key)
            with open(meta_path, 'r') as f:
                metadata = json.load(f)

            print(f"‚úÖ Loaded {len(chunks)} chunks from cache")
            print(f"   Cache created: {metadata['created_at']}")
            print(f"   Parameters: {metadata['parameters']}")

            return chunks

        except Exception as e:
            print(f"‚ö†Ô∏è  Error loading cache: {e}")
            return None

    def save_chunks(self, chunks: List[Dict], documents: List[str],
                   chunking_strategy: str, chunk_size: int, chunk_overlap: int,
                   embedding_model: str = None):
        """Save chunks to cache."""
        cache_key = self._compute_cache_key(documents, chunking_strategy, chunk_size, chunk_overlap, embedding_model)
        cache_path = self.get_cache_path(cache_key)
        meta_path = self.get_meta_path(cache_key)

        try:
            # Save chunks
            with open(cache_path, 'wb') as f:
                pickle.dump(chunks, f)

            # Save metadata
            metadata = {
                'cache_key': cache_key,
                'created_at': pd.Timestamp.now().isoformat(),
                'num_documents': len(documents),
                'num_chunks': len(chunks),
                'parameters': {
                    'chunking_strategy': chunking_strategy,
                    'chunk_size': chunk_size,
                    'chunk_overlap': chunk_overlap,
                    'embedding_model': embedding_model
                }
            }

            with open(meta_path, 'w') as f:
                json.dump(metadata, f, indent=2)

            print(f"üíæ Saved {len(chunks)} chunks to cache")
            print(f"   Cache key: {cache_key}")

        except Exception as e:
            print(f"‚ö†Ô∏è  Error saving cache: {e}")

    def list_cached_chunks(self):
        """List all cached chunk sets."""
        cache_files = list(self.cache_dir.glob("*_meta.json"))

        if not cache_files:
            print("No cached chunks found.")
            return

        print(f"Found {len(cache_files)} cached chunk sets:")
        print("-" * 80)

        for meta_file in cache_files:
            try:
                with open(meta_file, 'r') as f:
                    metadata = json.load(f)

                params = metadata['parameters']
                print(f"üìÅ Cache: {metadata['cache_key']}")
                print(f"   Created: {metadata['created_at']}")
                print(f"   Documents: {metadata['num_documents']}, Chunks: {metadata['num_chunks']}")
                print(f"   Strategy: {params['chunking_strategy']}, Size: {params['chunk_size']}, Overlap: {params['chunk_overlap']}")
                if params['embedding_model']:
                    print(f"   Embedding: {params['embedding_model']}")
                print()

            except Exception as e:
                print(f"‚ö†Ô∏è  Error reading {meta_file}: {e}")

    def clear_cache(self, confirm: bool = False):
        """Clear all cached chunks."""
        if not confirm:
            print("‚ö†Ô∏è  Use clear_cache(confirm=True) to actually clear the cache")
            return

        cache_files = list(self.cache_dir.glob("*"))
        for file in cache_files:
            file.unlink()

        print(f"üóëÔ∏è  Cleared {len(cache_files)} cache files")

# Initialize cache manager
chunk_cache = ChunkCacheManager()
print(f"üíæ Chunk cache initialized at: {chunk_cache.cache_dir}")
chunk_cache.list_cached_chunks()