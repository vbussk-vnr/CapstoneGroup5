{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ounwfsMCt8yB"
      },
      "source": [
        "# RAG System Evaluation Pipeline on RAGBench Dataset\n",
        "\n",
        "This notebook implements a complete RAG (Retrieval-Augmented Generation) system evaluation pipeline on the RAGBench dataset.\n",
        "\n",
        "## Project Overview\n",
        "- **Task 1**: Build evaluation pipeline for RAG system on RAGBench dataset\n",
        "- **Task 2**: Model implementation and optimization across different domains\n",
        "\n",
        "## Features\n",
        "- Document chunking strategies\n",
        "- Embedding models (domain-specific)\n",
        "- Vector stores (FAISS/Chroma)\n",
        "- Retrieval mechanisms (Dense/Sparse/Hybrid)\n",
        "- LLM response generation (Groq API)\n",
        "- LLM-as-judge for evaluation\n",
        "- Metrics computation (Context Relevance, Utilization, Completeness, Adherence)\n",
        "- Performance comparison with ground truth (RMSE, AUC-ROC)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "umBungI4t8yD"
      },
      "source": [
        "## 1. Install Required Packages\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hof2ayJnt8yD",
        "outputId": "6cf03d1d-5311-4434-eb6a-a4a66e453b6a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… All packages installed successfully!\n"
          ]
        }
      ],
      "source": [
        "# 1. Install required packages (Consolidated)\n",
        "%pip install -q \\\n",
        "  datasets \\\n",
        "  transformers \\\n",
        "  sentence-transformers \\\n",
        "  faiss-cpu \\\n",
        "  chromadb \\\n",
        "  groq \\\n",
        "  rank-bm25 \\\n",
        "  scikit-learn \\\n",
        "  scipy \\\n",
        "  pandas \\\n",
        "  pyyaml \\\n",
        "  loguru \\\n",
        "  tqdm \\\n",
        "  python-dotenv \\\n",
        "  opentelemetry-sdk==1.37.0 \\\n",
        "  opentelemetry-api==1.37.0 \\\n",
        "  opentelemetry-proto==1.37.0 \\\n",
        "  opentelemetry-exporter-otlp-proto-common==1.37.0\n",
        "\n",
        "print(\"âœ… All packages installed successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ujtXHwfrt8yE"
      },
      "source": [
        "## 2. Import Libraries\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ek954EaNt8yF",
        "outputId": "8e1bc844-a5d1-4da0-80f2-d63feaa959c9"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:torchao.kernel.intmm:Warning: Detected no triton, on systems without Triton certain kernels will not work\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import os\n",
        "import json\n",
        "import re\n",
        "import time\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import warnings\n",
        "import torch\n",
        "import nltk\n",
        "import faiss\n",
        "from typing import List, Dict, Tuple, Optional, Any\n",
        "from tqdm import tqdm\n",
        "from nltk.tokenize import sent_tokenize\n",
        "from datasets import load_dataset\n",
        "from sentence_transformers import SentenceTransformer, CrossEncoder\n",
        "from rank_bm25 import BM25Okapi\n",
        "from groq import Groq\n",
        "from sklearn.metrics import mean_squared_error, roc_auc_score\n",
        "from google.colab import userdata\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "nltk.download('punkt', quiet=True)\n",
        "nltk.download('punkt_tab', quiet=True) # Added to fix the LookupError\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RHwt1yxTubxz",
        "outputId": "dd2bf135-8016-441a-b14f-d1ab3157ef2e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… All libraries imported successfully!\n"
          ]
        }
      ],
      "source": [
        "# Step 2.1: Clear existing keys to ensure fresh setup\n",
        "if 'GROQ_API_KEY' in os.environ:\n",
        "     del os.environ['GROQ_API_KEY']\n",
        "     print(\"Environment variable cleared.\")\n",
        "print(\"âœ… All libraries imported successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FRKlWKkSt8yF"
      },
      "source": [
        "## 3. Configuration\n",
        "\n",
        "Set your configuration here:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pxbloa1Vt8yF",
        "outputId": "8f6063ad-9357-4171-f178-4cb865747abc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ðŸš€ System Initialized for RAGBench Domain: customer_support\n",
            "ðŸ“‚ Subset Focus: delucionqa, emanual, techqa\n",
            "ðŸ“Š Total Unique Experiment Combinations: 60\n",
            "ðŸ“ Results Path: /content/results\n"
          ]
        }
      ],
      "source": [
        "import itertools\n",
        "try:\n",
        "    GROQ_API_KEY = userdata.get('GROQ_API_KEY')\n",
        "except:\n",
        "    # Hardcoded fallback for immediate testing\n",
        "    GROQ_API_KEY = \"GROQ_API_KEY\"\n",
        "\n",
        "# ========== DATASET CONFIG ==========\n",
        "DATASET_REPOSITORY = \"galileo-ai/ragbench\"\n",
        "DOMAIN = \"customer_support\"\n",
        "SUBSETS = [\"delucionqa\", \"emanual\", \"techqa\"]\n",
        "SPLIT = \"test\"\n",
        "\n",
        "# ========== MODEL SELECTION ==========\n",
        "EMBEDDING_MODEL = \"BAAI/bge-large-en-v1.5\"\n",
        "CROSS_ENCODER_MODEL = \"BAAI/bge-reranker-v2-m3\"\n",
        "GENERATION_MODEL = \"llama-3.1-8b-instant\"\n",
        "JUDGE_MODEL = \"llama-3.3-70b-versatile\"\n",
        "\n",
        "CONFIG_MATRIX = {\n",
        "    \"RETRIEVAL_TYPE\": [\"hybrid\", \"vector\", \"bm25\"],\n",
        "    \"HYBRID_ALPHA\": [0.3, 0.5, 0.7],     # 0.3=Keyword Heavy, 0.7=Semantic Heavy\n",
        "    \"TOP_K_FINAL\": [5, 10, 15],         # Depth of context\n",
        "    \"USE_CROSS_ENCODER\": [True, False], # Importance of re-ranking\n",
        "    \"GENERATION_TEMPERATURE\": [0.0],    # Kept at 0.0 for maximum factuality in support\n",
        "    \"MAX_CONTEXT_LENGTH\": [2048, 4096]  # Test if longer context helps 'emanual'\n",
        "}\n",
        "\n",
        "# ========== HELPER: GENERATE ALL COMBINATIONS ==========\n",
        "def get_configurations(matrix):\n",
        "    \"\"\"Generates a list of all possible parameter combinations.\"\"\"\n",
        "    keys, values = zip(*matrix.items())\n",
        "    # Clean up: Alpha only matters if retrieval is 'hybrid'\n",
        "    configs = [dict(zip(keys, v)) for v in itertools.product(*values)]\n",
        "\n",
        "    # Filter logic: if RETRIEVAL_TYPE is not hybrid, HYBRID_ALPHA is irrelevant\n",
        "    unique_configs = []\n",
        "    for c in configs:\n",
        "        if c['RETRIEVAL_TYPE'] != 'hybrid':\n",
        "            c['HYBRID_ALPHA'] = None # Standardize non-hybrid configs\n",
        "        if c not in unique_configs:\n",
        "            unique_configs.append(c)\n",
        "    return unique_configs\n",
        "\n",
        "all_configs = get_configurations(CONFIG_MATRIX)\n",
        "\n",
        "# ========== OUTPUT ==========\n",
        "RESULTS_DIR = \"/content/results\"\n",
        "os.makedirs(RESULTS_DIR, exist_ok=True)\n",
        "\n",
        "print(f\"ðŸš€ System Initialized for RAGBench Domain: {DOMAIN}\")\n",
        "print(f\"ðŸ“‚ Subset Focus: {', '.join(SUBSETS)}\")\n",
        "print(f\"ðŸ“Š Total Unique Experiment Combinations: {len(all_configs)}\")\n",
        "print(f\"ðŸ“ Results Path: {RESULTS_DIR}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KKYnOQ9ObW2j"
      },
      "source": [
        "## 3.1 Experiment Configuration\n",
        "\n",
        "Define multiple experiments to run automatically:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wt45ZZb2bW2j",
        "outputId": "e964a0ca-c94d-40a5-a991-4af767d924b5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… Defined 5 optimized experiments.\n"
          ]
        }
      ],
      "source": [
        "# Updated Experiment Grid with specialized Customer Care strategies\n",
        "EXPERIMENTS = [\n",
        "    # --- BASELINE: Sentence-based (High Coherence for General Q&A) ---\n",
        "    {\n",
        "        \"name\": \"cs_sentence_hybrid_delucionqa\",\n",
        "        \"subset\": \"delucionqa\",\n",
        "        \"chunking_strategy\": \"sentence_based\",\n",
        "        \"retrieval_type\": \"hybrid\",\n",
        "        \"domain\": \"customer_support\",\n",
        "        \"embedding_model\": EMBEDDING_MODEL,\n",
        "        \"top_k_final\": 10,\n",
        "        \"use_cross_encoder\": True,\n",
        "        \"hybrid_alpha\": 0.5 # Balanced for intent + keywords\n",
        "    },\n",
        "\n",
        "    # --- TECHNICAL PRECISION: Fixed-size + BM25 (Best for Error Codes/Tech Specs) ---\n",
        "    {\n",
        "        \"name\": \"cs_fixed_bm25_techqa\",\n",
        "        \"subset\": \"techqa\",\n",
        "        \"chunking_strategy\": \"fixed_size\",\n",
        "        \"retrieval_type\": \"bm25\", # Testing pure keyword match for tech support\n",
        "        \"domain\": \"customer_support\",\n",
        "        \"embedding_model\": EMBEDDING_MODEL,\n",
        "        \"top_k_final\": 5, # Smaller K for higher precision\n",
        "        \"use_cross_encoder\": False\n",
        "    },\n",
        "\n",
        "    # --- COMPLEX STRUCTURE: Recursive (Best for Hierarchical Manuals) ---\n",
        "    {\n",
        "        \"name\": \"cs_recursive_hybrid_emanual\",\n",
        "        \"subset\": \"emanual\",\n",
        "        \"chunking_strategy\": \"recursive\",\n",
        "        \"retrieval_type\": \"hybrid\",\n",
        "        \"domain\": \"customer_support\",\n",
        "        \"embedding_model\": EMBEDDING_MODEL,\n",
        "        \"top_k_final\": 15, # Larger K to capture multi-part instructions\n",
        "        \"use_cross_encoder\": True,\n",
        "        \"hybrid_alpha\": 0.4 # Slightly favors keywords for part names\n",
        "    },\n",
        "\n",
        "    # --- SEMANTIC INTELLIGENCE: (Best for nuanced troubleshooting) ---\n",
        "    {\n",
        "        \"name\": \"cs_semantic_hybrid_emanual\",\n",
        "        \"subset\": \"emanual\",\n",
        "        \"chunking_strategy\": \"semantic\",\n",
        "        \"retrieval_type\": \"hybrid\",\n",
        "        \"domain\": \"customer_support\",\n",
        "        \"embedding_model\": EMBEDDING_MODEL,\n",
        "        \"top_k_final\": 10,\n",
        "        \"use_cross_encoder\": True,\n",
        "        \"hybrid_alpha\": 0.6 # Favors semantic meaning\n",
        "    },\n",
        "\n",
        "    # --- HYBRID RECOVERY: TechQA with Recursive + Hybrid ---\n",
        "    {\n",
        "        \"name\": \"cs_recursive_hybrid_techqa\",\n",
        "        \"subset\": \"techqa\",\n",
        "        \"chunking_strategy\": \"recursive\",\n",
        "        \"retrieval_type\": \"hybrid\",\n",
        "        \"domain\": \"customer_support\",\n",
        "        \"embedding_model\": EMBEDDING_MODEL,\n",
        "        \"top_k_final\": 10,\n",
        "        \"use_cross_encoder\": True,\n",
        "        \"hybrid_alpha\": 0.3 # Strong keyword bias (alpha 0.3) for tech troubleshooting\n",
        "    }\n",
        "]\n",
        "\n",
        "EXPERIMENTS = [exp for exp in EXPERIMENTS if not (exp['chunking_strategy'] == 'semantic' and exp['retrieval_type'] == 'bm25')]\n",
        "\n",
        "print(f\"âœ… Defined {len(EXPERIMENTS)} optimized experiments.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BSOrG3vGF6gv"
      },
      "outputs": [],
      "source": [
        "class ContextRepacker:\n",
        "    \"\"\"\n",
        "    Optimizes the placement of retrieved chunks within the context window\n",
        "    to mitigate 'Lost in the Middle' issues and stay within token limits.\n",
        "    \"\"\"\n",
        "    def __init__(self, max_context_length: int = 4096):\n",
        "        self.max_context_length = max_context_length\n",
        "\n",
        "    def _truncate_to_limit(self, candidates: List[str], max_chars: int) -> List[str]:\n",
        "        \"\"\"Ensures total character count stays within limits.\"\"\"\n",
        "        packed = []\n",
        "        cur_len = 0\n",
        "        for text in candidates:\n",
        "            if cur_len + len(text) > max_chars:\n",
        "                break\n",
        "            packed.append(text)\n",
        "            cur_len += len(text)\n",
        "        return packed\n",
        "\n",
        "    def pack_forward(self, candidates: List[str], max_chars: int) -> str:\n",
        "        \"\"\"Standard order: most relevant first. Good for 'techqa' exact matches.\"\"\"\n",
        "        selected = self._truncate_to_limit(candidates, max_chars)\n",
        "        return \"\\n\\n\".join(selected).strip()\n",
        "\n",
        "    def pack_reverse(self, candidates: List[str], max_chars: int) -> str:\n",
        "        \"\"\"Reverse order: most relevant last. Useful if the LLM has a strong recency bias.\"\"\"\n",
        "        selected = self._truncate_to_limit(list(reversed(candidates)), max_chars)\n",
        "        return \"\\n\\n\".join(selected).strip()\n",
        "\n",
        "    def pack_sides(self, candidates: List[str], max_chars: int) -> str:\n",
        "        \"\"\"\n",
        "        Alternates relevance between start and end.\n",
        "        Best for 'emanual' where instructions might span multiple non-sequential chunks.\n",
        "        \"\"\"\n",
        "        n = len(candidates)\n",
        "        out = []\n",
        "        left, right = 0, n - 1\n",
        "        cur_len = 0\n",
        "\n",
        "        while left <= right:\n",
        "            # Add from high relevance (start)\n",
        "            if cur_len + len(candidates[left]) < max_chars:\n",
        "                out.append(candidates[left])\n",
        "                cur_len += len(candidates[left])\n",
        "                left += 1\n",
        "\n",
        "            # Add from moderate relevance (end) to fill 'side'\n",
        "            if left <= right and cur_len + len(candidates[right]) < max_chars:\n",
        "                out.insert(len(out)//2, candidates[right]) # Insert in middle to push relevant to sides\n",
        "                cur_len += len(candidates[right])\n",
        "                right -= 1\n",
        "            else:\n",
        "                break\n",
        "        return \"\\n\\n\".join(out).strip()\n",
        "\n",
        "    def repack(self, candidates: List[str], strategy: str = \"sides\", max_chars: Optional[int] = None) -> str:\n",
        "        if max_chars is None:\n",
        "            max_chars = self.max_context_length * 3  # Roughly 3 chars per token fallback\n",
        "\n",
        "        strategy = strategy.lower()\n",
        "        if strategy == \"reverse\": return self.pack_reverse(candidates, max_chars)\n",
        "        if strategy == \"sides\": return self.pack_sides(candidates, max_chars)\n",
        "        return self.pack_forward(candidates, max_chars)\n",
        "\n",
        "# Initialize global repacker based on Config Matrix\n",
        "repacker = ContextRepacker(max_context_length=max(CONFIG_MATRIX[\"MAX_CONTEXT_LENGTH\"]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X5GOKqJQt8yF"
      },
      "source": [
        "## 4. Helper Functions\n",
        "\n",
        "### 4.1 Document Chunking\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3u-810wXt8yF",
        "outputId": "a3012c46-b109-4423-d174-e397cb517a33"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… DocumentChunker initialized with all strategies.\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "class DocumentChunker:\n",
        "    \"\"\"\n",
        "    Intelligently splits documents into manageable chunks based on the experiment\n",
        "    configuration. Supports technical manual structures and semantic shifts.\n",
        "    \"\"\"\n",
        "    def __init__(self, strategy: str, embedding_model=None, config: Dict = None):\n",
        "        self.strategy = strategy\n",
        "        self.embedding_model = embedding_model\n",
        "        self.config = config or {}\n",
        "\n",
        "        # Default hyper-parameters for Customer Support domain\n",
        "        self.chunk_size = self.config.get(\"chunk_size\", 500)      # Words/Tokens approx\n",
        "        self.chunk_overlap = self.config.get(\"chunk_overlap\", 50) # Context preservation\n",
        "        self.breakpoint_percentile = self.config.get(\"breakpoint_percentile\", 95)\n",
        "\n",
        "    def chunk_documents(self, documents: List[str]) -> List[Dict]:\n",
        "        \"\"\"Processes a list of documents and returns structured chunks.\"\"\"\n",
        "        all_chunks = []\n",
        "        for doc_id, doc_text in enumerate(documents):\n",
        "            # Route to specific strategy\n",
        "            if self.strategy == \"semantic\":\n",
        "                chunks = self._semantic_chunking(doc_text)\n",
        "            elif self.strategy == \"recursive\":\n",
        "                chunks = self._recursive_chunking(doc_text)\n",
        "            elif self.strategy == \"sentence_based\":\n",
        "                chunks = self._sentence_based_chunking(doc_text)\n",
        "            else: # fixed_size baseline\n",
        "                chunks = self._fixed_size_chunking(doc_text)\n",
        "\n",
        "            for chunk_id, text in enumerate(chunks):\n",
        "                all_chunks.append({\n",
        "                    \"chunk_id\": f\"doc_{doc_id}_ch_{chunk_id}\",\n",
        "                    \"text\": text,\n",
        "                    \"doc_id\": doc_id,\n",
        "                    \"strategy\": self.strategy\n",
        "                })\n",
        "        return all_chunks\n",
        "\n",
        "    def _fixed_size_chunking(self, text: str) -> List[str]:\n",
        "        \"\"\"Simple word-count based split. High speed, low context awareness.\"\"\"\n",
        "        words = text.split()\n",
        "        chunks = []\n",
        "        step = max(1, self.chunk_size - self.chunk_overlap)\n",
        "        for i in range(0, len(words), step):\n",
        "            chunk = \" \".join(words[i : i + self.chunk_size])\n",
        "            if chunk.strip():\n",
        "                chunks.append(chunk)\n",
        "        return chunks\n",
        "\n",
        "    def _sentence_based_chunking(self, text: str) -> List[str]:\n",
        "        \"\"\"Splits by NLTK sentences to avoid mid-sentence breaks.\"\"\"\n",
        "        sentences = sent_tokenize(text)\n",
        "        chunks, current_chunk, current_len = [], [], 0\n",
        "        for sent in sentences:\n",
        "            sent_words = len(sent.split())\n",
        "            if current_len + sent_words > self.chunk_size and current_chunk:\n",
        "                chunks.append(\" \".join(current_chunk))\n",
        "                current_chunk, current_len = [], 0\n",
        "            current_chunk.append(sent)\n",
        "            current_len += sent_words\n",
        "        if current_chunk:\n",
        "            chunks.append(\" \".join(current_chunk))\n",
        "        return chunks\n",
        "\n",
        "    def _recursive_chunking(self, text: str) -> List[str]:\n",
        "        \"\"\"\n",
        "        Hierarchical split (Paragraph -> Sentence).\n",
        "        Ideal for 'emanual' where structure (steps) must be preserved.\n",
        "        \"\"\"\n",
        "        # First split by double newlines (paragraphs)\n",
        "        paragraphs = [p.strip() for p in text.split(\"\\n\\n\") if p.strip()]\n",
        "        final_chunks = []\n",
        "\n",
        "        for para in paragraphs:\n",
        "            if len(para.split()) <= self.chunk_size:\n",
        "                final_chunks.append(para)\n",
        "            else:\n",
        "                # If paragraph is too large, split into sentences\n",
        "                sentences = sent_tokenize(para)\n",
        "                curr, curr_len = [], 0\n",
        "                for s in sentences:\n",
        "                    s_len = len(s.split())\n",
        "                    if curr_len + s_len > self.chunk_size and curr:\n",
        "                        final_chunks.append(\" \".join(curr))\n",
        "                        curr, curr_len = [], 0\n",
        "                    curr.append(s)\n",
        "                    curr_len += s_len\n",
        "                if curr: final_chunks.append(\" \".join(curr))\n",
        "        return final_chunks\n",
        "\n",
        "    def _semantic_chunking(self, text: str) -> List[str]:\n",
        "        \"\"\"\n",
        "        Uses embeddings to detect topical shifts.\n",
        "        Highly effective for 'delucionqa' conversational data.\n",
        "        \"\"\"\n",
        "        sentences = sent_tokenize(text)\n",
        "        if len(sentences) < 2:\n",
        "            return [text]\n",
        "\n",
        "        # 1. Embed sentences using the model from Step 3\n",
        "        embeddings = self.embedding_model.encode(sentences)\n",
        "\n",
        "        # 2. Calculate distances between consecutive sentences\n",
        "        distances = []\n",
        "        for i in range(len(embeddings) - 1):\n",
        "            sim = cosine_similarity([embeddings[i]], [embeddings[i+1]])[0][0]\n",
        "            distances.append(1 - sim) # Higher distance = Semantic shift\n",
        "\n",
        "        # 3. Determine threshold for breakpoints\n",
        "        threshold = np.percentile(distances, self.breakpoint_percentile)\n",
        "\n",
        "        # 4. Group sentences\n",
        "        chunks = []\n",
        "        current_chunk = [sentences[0]]\n",
        "        for i, dist in enumerate(distances):\n",
        "            if dist > threshold:\n",
        "                chunks.append(\" \".join(current_chunk))\n",
        "                current_chunk = [sentences[i+1]]\n",
        "            else:\n",
        "                current_chunk.append(sentences[i+1])\n",
        "\n",
        "        if current_chunk:\n",
        "            chunks.append(\" \".join(current_chunk))\n",
        "        return chunks\n",
        "\n",
        "print(\"âœ… DocumentChunker initialized with all strategies.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ATspWO0t8yG"
      },
      "source": [
        "### 4.2 Embedding Model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dfhHmBSSt8yG"
      },
      "outputs": [],
      "source": [
        "def load_models_for_experiment(exp: Dict) -> Tuple[Optional[SentenceTransformer], Optional[int], Optional[CrossEncoder]]:\n",
        "    \"\"\"\n",
        "    Dynamically loads embedding and reranking models based on experiment requirements.\n",
        "    \"\"\"\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    embedding_model, embedding_dim, cross_encoder = None, None, None\n",
        "\n",
        "    # Load Embedding Model for Vector/Hybrid retrieval or Semantic Chunking\n",
        "    if exp.get(\"retrieval_type\") in [\"vector\", \"hybrid\"] or exp.get(\"chunking_strategy\") == \"semantic\":\n",
        "        model_name = exp.get(\"embedding_model\", EMBEDDING_MODEL)\n",
        "        embedding_model = SentenceTransformer(model_name, device=device)\n",
        "        embedding_dim = embedding_model.get_sentence_embedding_dimension()\n",
        "        print(f\"  - Loaded Embedding Model: {model_name} (Dim: {embedding_dim})\")\n",
        "\n",
        "    # Load Cross-Encoder for Reranking\n",
        "    if exp.get(\"use_cross_encoder\", False):\n",
        "        ce_name = CROSS_ENCODER_MODEL\n",
        "        cross_encoder = CrossEncoder(ce_name, device=device)\n",
        "        print(f\"  - Loaded Cross-Encoder: {ce_name}\")\n",
        "\n",
        "    return embedding_model, embedding_dim, cross_encoder"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1hzCkjmYt8yG"
      },
      "source": [
        "### 4.3 Vector Store (FAISS)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x8STX4o8t8yG"
      },
      "outputs": [],
      "source": [
        "class FAISSVectorStore:\n",
        "    \"\"\"\n",
        "    FAISS-based vector database for high-speed dense retrieval.\n",
        "    \"\"\"\n",
        "    def __init__(self, embedding_dim: int, index_type: str = \"InnerProduct\"):\n",
        "        if index_type == \"InnerProduct\":\n",
        "            self.index = faiss.IndexFlatIP(embedding_dim)\n",
        "        else:\n",
        "            self.index = faiss.IndexFlatL2(embedding_dim)\n",
        "\n",
        "        self.index_type = index_type\n",
        "        self.metadata = []\n",
        "\n",
        "    def add_documents(self, embeddings: np.ndarray, metadatas: List[Dict]):\n",
        "        embeddings = embeddings.astype(\"float32\")\n",
        "        if self.index_type == \"InnerProduct\":\n",
        "            faiss.normalize_L2(embeddings)\n",
        "        self.index.add(embeddings)\n",
        "        self.metadata.extend(metadatas)\n",
        "\n",
        "    def search(self, query_embedding: np.ndarray, top_k: int) -> List[Dict]:\n",
        "        q_emb = query_embedding.reshape(1, -1).astype(\"float32\")\n",
        "        if self.index_type == \"InnerProduct\":\n",
        "            faiss.normalize_L2(q_emb)\n",
        "\n",
        "        scores, indices = self.index.search(q_emb, top_k)\n",
        "        results = []\n",
        "        for score, idx in zip(scores[0], indices[0]):\n",
        "            if idx != -1:\n",
        "                hit = self.metadata[idx].copy()\n",
        "                hit[\"dense_score\"] = float(score)\n",
        "                results.append(hit)\n",
        "        return results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "15h8cH1wt8yG"
      },
      "source": [
        "### 4.4 Retrieval Mechanisms with Cross-Encoder Reranking\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6cw8IOREt8yG"
      },
      "outputs": [],
      "source": [
        "class HybridRetrieverWithRerank:\n",
        "    \"\"\"\n",
        "    Unified retrieval engine that manages BM25, Vector Search,\n",
        "    and Cross-Encoder Reranking logic.\n",
        "    \"\"\"\n",
        "    def __init__(self, config: Dict, vector_store: FAISSVectorStore,\n",
        "                 embedding_model: SentenceTransformer, cross_encoder: Optional[CrossEncoder]):\n",
        "        self.retrieval_type = config.get(\"retrieval_type\", \"hybrid\")\n",
        "        self.vector_store = vector_store\n",
        "        self.embedding_model = embedding_model\n",
        "        self.cross_encoder = cross_encoder\n",
        "        self.hybrid_alpha = config.get(\"hybrid_alpha\", 0.5)\n",
        "        self.bm25 = None\n",
        "        self.corpus_chunks = []\n",
        "\n",
        "    def set_corpus(self, chunks: List[Dict]):\n",
        "        \"\"\"Initializes BM25 index with the provided document chunks.\"\"\"\n",
        "        self.corpus_chunks = chunks\n",
        "        tokenized_corpus = [c[\"text\"].lower().split() for c in chunks]\n",
        "        self.bm25 = BM25Okapi(tokenized_corpus)\n",
        "\n",
        "    def retrieve(self, query: str, top_k_final: int = 10) -> List[Dict]:\n",
        "        # Step 1: Broad Retrieval (get a pool of candidates)\n",
        "        pool_size = 50\n",
        "        candidates = {}\n",
        "\n",
        "        # A. Sparse Retrieval (BM25)\n",
        "        if self.retrieval_type in [\"bm25\", \"hybrid\"]:\n",
        "            bm25_scores = self.bm25.get_scores(query.lower().split())\n",
        "            top_indices = np.argsort(bm25_scores)[::-1][:pool_size]\n",
        "            for i in top_indices:\n",
        "                hit = self.corpus_chunks[i].copy()\n",
        "                hit[\"sparse_score\"] = float(bm25_scores[i])\n",
        "                candidates[hit[\"chunk_id\"]] = hit\n",
        "\n",
        "        # B. Dense Retrieval (Vector)\n",
        "        if self.retrieval_type in [\"vector\", \"hybrid\"]:\n",
        "            q_emb = self.embedding_model.encode(query, convert_to_numpy=True)\n",
        "            vector_hits = self.vector_store.search(q_emb, pool_size)\n",
        "            for hit in vector_hits:\n",
        "                cid = hit[\"chunk_id\"]\n",
        "                if cid in candidates:\n",
        "                    candidates[cid][\"dense_score\"] = hit[\"dense_score\"]\n",
        "                else:\n",
        "                    candidates[cid] = hit\n",
        "\n",
        "        # Step 2: Hybrid Scoring (Alpha Blending)\n",
        "        hits = list(candidates.values())\n",
        "        if self.retrieval_type == \"hybrid\":\n",
        "            # Simple min-max norm for combining scores\n",
        "            for h in hits:\n",
        "                s_score = h.get(\"sparse_score\", 0)\n",
        "                d_score = h.get(\"dense_score\", 0)\n",
        "                # Weighted blend\n",
        "                h[\"combined_score\"] = (self.hybrid_alpha * d_score) + ((1 - self.hybrid_alpha) * s_score)\n",
        "            hits = sorted(hits, key=lambda x: x[\"combined_score\"], reverse=True)\n",
        "        else:\n",
        "            # Sort by whatever score is available\n",
        "            score_key = \"sparse_score\" if self.retrieval_type == \"bm25\" else \"dense_score\"\n",
        "            hits = sorted(hits, key=lambda x: x.get(score_key, 0), reverse=True)\n",
        "\n",
        "        # Step 3: Cross-Encoder Reranking (Precision Layer)\n",
        "        if self.cross_encoder and hits:\n",
        "            # We rerank the top 25 to find the best 10\n",
        "            rerank_pool = hits[:25]\n",
        "            ce_inputs = [[query, h[\"text\"]] for h in rerank_pool]\n",
        "            ce_scores = self.cross_encoder.predict(ce_inputs)\n",
        "            for h, s in zip(rerank_pool, ce_scores):\n",
        "                h[\"ce_score\"] = float(s)\n",
        "            hits = sorted(rerank_pool, key=lambda x: x[\"ce_score\"], reverse=True)\n",
        "\n",
        "        return hits[:top_k_final]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E9JZ2LYlcP4s",
        "outputId": "44540028-eec0-4178-cd89-e9766e0e75ef"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… Step 5 (Retrieval & Generation) complete. Pipeline flow validated!\n"
          ]
        }
      ],
      "source": [
        "# Global Groq Client\n",
        "groq_client = Groq(api_key=GROQ_API_KEY)\n",
        "\n",
        "def generate_response(question: str, retrieved_chunks: List[Dict], exp_config: Dict):\n",
        "    \"\"\"\n",
        "    Uses Groq to generate a final support answer based on repacked context.\n",
        "    \"\"\"\n",
        "    # 1. Repack context using the strategy defined for the domain\n",
        "    # Use 'sides' for manuals, 'forward' for troubleshooting\n",
        "    repack_strategy = \"sides\" if exp_config.get(\"subset\") == \"emanual\" else \"forward\"\n",
        "    max_len = exp_config.get(\"MAX_CONTEXT_LENGTH\", 3000)\n",
        "\n",
        "    context_text = repacker.repack(\n",
        "        [c[\"text\"] for c in retrieved_chunks],\n",
        "        strategy=repack_strategy,\n",
        "        max_chars=max_len\n",
        "    )\n",
        "\n",
        "    # 2. Construct Domain-Specific Prompts\n",
        "    system_prompt = (\n",
        "        \"You are an expert Customer Support Assistant. \"\n",
        "        \"Your task is to provide accurate, helpful answers based ONLY on the provided context. \"\n",
        "        \"If the answer is not in the context, state that you do not have enough information.\"\n",
        "    )\n",
        "\n",
        "    user_prompt = f\"### CONTEXT ###\\n{context_text}\\n\\n### QUESTION ###\\n{question}\\n\\n### ANSWER ###\"\n",
        "\n",
        "    try:\n",
        "        response = groq_client.chat.completions.create(\n",
        "            model=GENERATION_MODEL,\n",
        "            messages=[\n",
        "                {\"role\": \"system\", \"content\": system_prompt},\n",
        "                {\"role\": \"user\", \"content\": user_prompt}\n",
        "            ],\n",
        "            temperature=0.0, # Factuality first\n",
        "            max_tokens=1024\n",
        "        )\n",
        "        return response.choices[0].message.content.strip()\n",
        "    except Exception as e:\n",
        "        return f\"Pipeline Error: {str(e)}\"\n",
        "\n",
        "print(\"âœ… Step 5 (Retrieval & Generation) complete. Pipeline flow validated!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T-0HExoMt8yH"
      },
      "source": [
        "### 4.5 LLM Generator and Judge\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "slIcnVBzt8yH",
        "outputId": "7c629d73-44f2-415e-b6d9-2617f04f0bd5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… Step Complete: LLM-as-Judge engine is ready.\n"
          ]
        }
      ],
      "source": [
        "# Configuration Constants\n",
        "JUDGE_TEMPERATURE = 0.0\n",
        "GENERATION_MODEL = \"llama-3.3-70b-versatile\"\n",
        "JUDGE_MODEL = \"llama-3.3-70b-versatile\"\n",
        "GENERATION_TEMPERATURE = 0.7\n",
        "GENERATION_MAX_TOKENS = 1024\n",
        "\n",
        "def generate_response(question: str, retrieved_chunks: List[Dict], model: str = GENERATION_MODEL) -> str:\n",
        "    \"\"\"Generate response using LLM with side-packed context to mitigate lost-in-the-middle.\"\"\"\n",
        "\n",
        "    # 1. Prepare context using the Repacker defined in Step 3.2\n",
        "    raw_texts = [c[\"text\"] for c in retrieved_chunks]\n",
        "    context_text = repacker.repack(raw_texts, strategy=\"sides\", max_chars=3000)\n",
        "\n",
        "    system_prompt = \"\"\"You are a helpful customer support assistant.\n",
        "    Use only the information from the context documents to answer the question.\n",
        "    If the context doesn't contain the answer, explicitly state that you don't have enough information.\"\"\"\n",
        "\n",
        "    user_prompt = f\"\"\"Context Documents:\n",
        "{context_text}\n",
        "\n",
        "Question: {question}\n",
        "\n",
        "Answer:\"\"\"\n",
        "\n",
        "    try:\n",
        "        response = groq_client.chat.completions.create(\n",
        "            model=model,\n",
        "            messages=[\n",
        "                {\"role\": \"system\", \"content\": system_prompt},\n",
        "                {\"role\": \"user\", \"content\": user_prompt}\n",
        "            ],\n",
        "            temperature=GENERATION_TEMPERATURE,\n",
        "            max_tokens=GENERATION_MAX_TOKENS\n",
        "        )\n",
        "        return response.choices[0].message.content.strip()\n",
        "    except Exception as e:\n",
        "        print(f\"âŒ Error generating response: {e}\")\n",
        "        return \"\"\n",
        "\n",
        "def format_docs_for_judge(chunks: List[Dict]) -> str:\n",
        "    \"\"\"Formats chunks into a numbered sentence list (0a, 0b) for the judge.\"\"\"\n",
        "    formatted = []\n",
        "    for i, chunk in enumerate(chunks):\n",
        "        sentences = sent_tokenize(chunk['text'])\n",
        "        for j, sent in enumerate(sentences):\n",
        "            key = f\"{i}{chr(97 + j)}\" # e.g., 0a, 0b\n",
        "            formatted.append(f\"{key}. {sent}\")\n",
        "    return \"\\n\".join(formatted)\n",
        "\n",
        "def format_response_for_judge(response: str) -> str:\n",
        "    \"\"\"Formats the LLM response into keys (a., b.) for the judge.\"\"\"\n",
        "    sentences = sent_tokenize(response)\n",
        "    return \"\\n\".join([f\"{chr(97 + i)}. {sent}\" for i, sent in enumerate(sentences)])\n",
        "\n",
        "def extract_attributes(question: str, retrieved_chunks: List[Dict], response: str) -> Dict:\n",
        "    \"\"\"Extract evaluation attributes using LLM-as-judge with JSON mode.\"\"\"\n",
        "\n",
        "    if not response:\n",
        "        return {\"overall_supported\": False, \"explanation\": \"No response generated\"}\n",
        "\n",
        "    # Format inputs for the detailed Judge Prompt\n",
        "    formatted_docs = format_docs_for_judge(retrieved_chunks)\n",
        "    formatted_response = format_response_for_judge(response)\n",
        "\n",
        "    # Prompt remains identical to your provided schema\n",
        "    prompt = f\"\"\"[I asked someone to answer a question based on one or more documents.\n",
        "Your task is to review their response and assess whether or not each sentence\n",
        "in that response is supported by text in the documents. And if so, which\n",
        "sentences in the documents provide that support. You will also tell me which\n",
        "of the documents contain useful information for answering the question, and\n",
        "which of the documents the answer was sourced from.\n",
        "Here are the documents, each of which is split into sentences. Alongside each\n",
        "sentence is associated key, such as â€™0a.â€™ or â€™0b.â€™ that you can use to refer\n",
        "to it:]\n",
        "\n",
        "Documents:\n",
        "{formatted_docs}\n",
        "\n",
        "Question:\n",
        "{question}\n",
        "'''\n",
        "Here is their response, split into sentences. Alongside each sentence is\n",
        "associated key, such as â€™a.â€™ or â€™b.â€™ that you can use to refer to it. Note\n",
        "that these keys are unique to the response, and are not related to the keys\n",
        "in the documents:\n",
        "'''\n",
        "Response:\n",
        "{formatted_response}\n",
        "\n",
        "You must respond with a JSON object matching this schema:\n",
        "''' {{\n",
        "  \"relevance_explanation\": string,\n",
        "  \"all_relevant_sentence_keys\": [string],\n",
        "  \"overall_supported_explanation\": string,\n",
        "  \"overall_supported\": boolean,\n",
        "  \"sentence_support_information\": [\n",
        "    {{\n",
        "      \"response_sentence_key\": string,\n",
        "      \"explanation\": string,\n",
        "      \"supporting_sentence_keys\": [string],\n",
        "      \"fully_supported\": boolean\n",
        "    }}\n",
        "  ],\n",
        "  \"all_utilized_sentence_keys\": [string]\n",
        "}}\n",
        "'''\n",
        "The relevance_explanation field is a string explaining which documents\n",
        "contain useful information for answering the question. Provide a step-by-step\n",
        "breakdown of information provided in the documents and how it is useful for\n",
        "answering the question.\n",
        "The all_relevant_sentence_keys field is a list of all document sentences keys\n",
        "(e.g. â€™0aâ€™) that are revant to the question. Include every sentence that is\n",
        "useful and relevant to the question, even if it was not used in the response,\n",
        "or if only parts of the sentence are useful. Ignore the provided response when\n",
        "making this judgement and base your judgement solely on the provided documents\n",
        "and question. Omit sentences that, if removed from the document, would not\n",
        "impact someoneâ€™s ability to answer the question.\n",
        "The overall_supported_explanation field is a string explaining why the response\n",
        "*as a whole* is or is not supported by the documents. In this field, provide a\n",
        "step-by-step breakdown of the claims made in the response and the support (or\n",
        "lack thereof) for those claims in the documents. Begin by assessing each claim\n",
        "separately, one by one; donâ€™t make any remarks about the response as a whole\n",
        "until you have assessed all the claims in isolation.\n",
        "The overall_supported field is a boolean indicating whether the response as a\n",
        "whole is supported by the documents. This value should reflect the conclusion\n",
        "you drew at the end of your step-by-step breakdown in overall_supported_explanation.\n",
        "In the sentence_support_information field, provide information about the support\n",
        "*for each sentence* in the response.\n",
        "The sentence_support_information field is a list of objects, one for each sentence\n",
        "in the response. Each object MUST have the following fields:\n",
        "- response_sentence_key: a string identifying the sentence in the response.\n",
        "This key is the same as the one used in the response above.\n",
        "- explanation: a string explaining why the sentence is or is not supported by the\n",
        "documents.\n",
        "- supporting_sentence_keys: keys (e.g. â€™0aâ€™) of sentences from the documents that\n",
        "support the response sentence. If the sentence is not supported, this list MUST\n",
        "be empty. If the sentence is supported, this list MUST contain one or more keys.\n",
        "In special cases where the sentence is supported, but not by any specific sentence,\n",
        "you can use the string \"supported_without_sentence\" to indicate that the sentence\n",
        "is generally supported by the documents. Consider cases where the sentence is\n",
        "expressing inability to answer the question due to lack of relevant information in\n",
        "the provided contex as \"supported_without_sentence\". In cases where the sentence\n",
        "is making a general statement (e.g. outlining the steps to produce an answer, or\n",
        "summarizing previously stated sentences, or a transition sentence), use the\n",
        "sting \"general\".In cases where the sentence is correctly stating a well-known fact,\n",
        "like a mathematical formula, use the string \"well_known_fact\". In cases where the\n",
        "sentence is performing numerical reasoning (e.g. addition, multiplication), use\n",
        "the string \"numerical_reasoning\".\n",
        "- fully_supported: a boolean indicating whether the sentence is fully supported by\n",
        "the documents.\n",
        "  - This value should reflect the conclusion you drew at the end of your step-by-step\n",
        "  breakdown in explanation.\n",
        "  - If supporting_sentence_keys is an empty list, then fully_supported must be false.\n",
        "- Otherwise, use fully_supported to clarify whether everything in the response\n",
        "  sentence is fully supported by the document text indicated in supporting_sentence_keys\n",
        "  (fully_supported = true), or whether the sentence is only partially or incompletely\n",
        "  supported by that document text (fully_supported = false).\n",
        "The all_utilized_sentence_keys field is a list of all sentences keys (e.g. â€™0aâ€™) that\n",
        "were used to construct the answer. Include every sentence that either directly supported\n",
        "the answer, or was implicitly used to construct the answer, even if it was not used\n",
        "in its entirety. Omit sentences that were not used, and could have been removed from\n",
        "the documents without affecting the answer.\n",
        "You must respond with a valid JSON string.  Use escapes for quotes, e.g. '\"', and\n",
        "newlines, e.g. '\\n'. Do not write anything before or after the JSON string. Do not\n",
        "wrap the JSON string in backticks like ''' or '''json.\n",
        "As a reminder: your task is to review the response and assess which documents contain\n",
        "useful information pertaining to the question, and how each sentence in the response\n",
        "is supported by the text in the documents.\"\"\"\n",
        "\n",
        "    try:\n",
        "        judge_response = groq_client.chat.completions.create(\n",
        "            model=JUDGE_MODEL,\n",
        "            messages=[\n",
        "                {\"role\": \"system\", \"content\": \"You are an expert evaluator for RAG systems. Respond ONLY in valid JSON.\"},\n",
        "                {\"role\": \"user\", \"content\": prompt}\n",
        "            ],\n",
        "            temperature=JUDGE_TEMPERATURE,\n",
        "            max_tokens=2048,\n",
        "            response_format={\"type\": \"json_object\"}\n",
        "        )\n",
        "\n",
        "        result_text = judge_response.choices[0].message.content.strip()\n",
        "        return json.loads(result_text)\n",
        "    except Exception as e:\n",
        "        print(f\"âŒ Evaluation Error: {e}\")\n",
        "        return {\"overall_supported\": False, \"sentence_support_information\": []}\n",
        "\n",
        "print(\"âœ… Step Complete: LLM-as-Judge engine is ready.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SrTrNveRt8yL"
      },
      "source": [
        "### 4.6 Metrics Computation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BWJi5Bbpt8yL",
        "outputId": "716db6e4-b864-48c2-b476-65474687268b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… Step 5.7 Complete: Metrics aligned with LLM-Judge attributes.\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "from typing import List, Dict\n",
        "\n",
        "def count_sentences(documents: List[str]) -> int:\n",
        "    \"\"\"\n",
        "    Counts total sentences in the retrieved context using the same logic\n",
        "    as the Judge to ensure denominator consistency.\n",
        "    \"\"\"\n",
        "    total = 0\n",
        "    for doc in documents:\n",
        "        # Using a slightly more robust split that matches Step 5.6 logic\n",
        "        sentences = re.split(r\"(?<=[.!?])\\s+\", doc)\n",
        "        total += len([s for s in sentences if s.strip()])\n",
        "    return total\n",
        "\n",
        "# ---------- Optimized Customer Care Metrics ----------\n",
        "\n",
        "def compute_context_precision(relevant_keys: List[str], total_sentences: int) -> float:\n",
        "    \"\"\"\n",
        "    Metric: Contextual Relevancy / Precision\n",
        "    In Support: High score means the retriever is 'Signal' focused.\n",
        "    Low score means 'Noise' (the retriever brought in 50 sentences but only 2 were useful).\n",
        "    \"\"\"\n",
        "    if total_sentences == 0:\n",
        "        return 0.0\n",
        "    return len(relevant_keys) / total_sentences\n",
        "\n",
        "\n",
        "def compute_faithfulness(sentence_support_info: List[Dict]) -> float:\n",
        "    \"\"\"\n",
        "    Metric: Faithfulness (Groundedness)\n",
        "    In Support: Prevents 'Creative Hallucinations'.\n",
        "    Measures % of the generated response sentences that are fully backed by docs.\n",
        "    \"\"\"\n",
        "    if not sentence_support_info:\n",
        "        return 0.0\n",
        "\n",
        "    # We count 'fully_supported' as per the Judge's boolean verdict\n",
        "    supported_count = sum(1 for s in sentence_support_info if s.get(\"fully_supported\", False))\n",
        "    return supported_count / len(sentence_support_info)\n",
        "\n",
        "\n",
        "def compute_completeness(relevant_keys: List[str], utilized_keys: List[str]) -> float:\n",
        "    \"\"\"\n",
        "    Metric: Context Recall (Completeness)\n",
        "    In Support: Did the bot leave out the 'Warning' or 'Safety' steps?\n",
        "    Measures if the bot used all relevant information found in retrieval.\n",
        "    \"\"\"\n",
        "    rel_set = set(relevant_keys)\n",
        "    util_set = set(utilized_keys)\n",
        "\n",
        "    if not rel_set:\n",
        "        # If there was no relevant info in context, and the bot (correctly)\n",
        "        # said \"I don't know\", completeness is 1.0.\n",
        "        return 1.0\n",
        "\n",
        "    # Intersection of what was relevant vs what was actually used in the answer\n",
        "    return len(rel_set & util_set) / len(rel_set)\n",
        "\n",
        "\n",
        "# ---------- Aggregate Execution ----------\n",
        "\n",
        "def compute_all_metrics(attributes: Dict, retrieved_chunks: List[Dict]) -> Dict[str, float]:\n",
        "    \"\"\"\n",
        "    Calculates the final quality suite for a customer support interaction.\n",
        "    Input 'attributes' comes directly from the extract_attributes (Step 5.6).\n",
        "    \"\"\"\n",
        "    raw_texts = [c[\"text\"] for c in retrieved_chunks]\n",
        "    total_context_sentences = count_sentences(raw_texts)\n",
        "\n",
        "    relevant_keys = attributes.get(\"all_relevant_sentence_keys\", [])\n",
        "    utilized_keys = attributes.get(\"all_utilized_sentence_keys\", [])\n",
        "    support_info = attributes.get(\"sentence_support_information\", [])\n",
        "\n",
        "    # Calculate metrics\n",
        "    precision = compute_context_precision(relevant_keys, total_context_sentences)\n",
        "    faithfulness = compute_faithfulness(support_info)\n",
        "    completeness = compute_completeness(relevant_keys, utilized_keys)\n",
        "\n",
        "    return {\n",
        "        \"retrieval_precision\": round(precision, 4),\n",
        "        \"faithfulness_score\": round(faithfulness, 4),\n",
        "        \"answer_completeness\": round(completeness, 4),\n",
        "        \"overall_score\": round((precision + faithfulness + completeness) / 3, 4)\n",
        "    }\n",
        "\n",
        "print(\"âœ… Step 5.7 Complete: Metrics aligned with LLM-Judge attributes.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G1SGymSMt8yL"
      },
      "source": [
        "## 5. Load RAGBench Dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 248
        },
        "id": "aTvFxJJYt8yL",
        "outputId": "050eb4d9-8bbf-48aa-e2be-3606c3482e1d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ðŸ“¡ Accessing RAGBench repository: galileo-ai/ragbench...\n",
            "   -> Loading Subset: delucionqa | Split: test\n",
            "   -> Loading Subset: emanual | Split: test\n",
            "   -> Loading Subset: techqa | Split: test\n",
            "âœ… Data Ready! Loaded 315 unique questions across 3 domains.\n",
            "âœ‚ï¸ Deduped 315 overlapping entries.\n"
          ]
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "summary": "{\n  \"name\": \"display(df_ragbench[['subset_source', 'question', 'documents']]\",\n  \"rows\": 3,\n  \"fields\": [\n    {\n      \"column\": \"subset_source\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"delucionqa\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"question\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          \"What if I fail to latch the tailgate properly?\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"documents\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}",
              "type": "dataframe"
            },
            "text/html": [
              "\n",
              "  <div id=\"df-285dc951-fd40-44c4-b8a7-66b3816e8d13\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>subset_source</th>\n",
              "      <th>question</th>\n",
              "      <th>documents</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>delucionqa</td>\n",
              "      <td>What if I fail to latch the tailgate properly?</td>\n",
              "      <td>[ Closing To close the tailgate, lift upward u...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>delucionqa</td>\n",
              "      <td>What kind of safety features are implemented i...</td>\n",
              "      <td>[ Some of the most important safety features i...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>delucionqa</td>\n",
              "      <td>When will the Automatic SOS be triggered?</td>\n",
              "      <td>[ Automatic SOS â€” If Equipped Automatic SOS is...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-285dc951-fd40-44c4-b8a7-66b3816e8d13')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-285dc951-fd40-44c4-b8a7-66b3816e8d13 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-285dc951-fd40-44c4-b8a7-66b3816e8d13');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "text/plain": [
              "  subset_source                                           question  \\\n",
              "0    delucionqa     What if I fail to latch the tailgate properly?   \n",
              "1    delucionqa  What kind of safety features are implemented i...   \n",
              "2    delucionqa          When will the Automatic SOS be triggered?   \n",
              "\n",
              "                                           documents  \n",
              "0  [ Closing To close the tailgate, lift upward u...  \n",
              "1  [ Some of the most important safety features i...  \n",
              "2  [ Automatic SOS â€” If Equipped Automatic SOS is...  "
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from datasets import load_dataset\n",
        "\n",
        "# Configuration from previous steps\n",
        "# DATASET_REPOSITORY = \"galileo-ai/ragbench\"\n",
        "# SUBSETS = [\"techqa\", \"emanual\"]\n",
        "# SPLIT = \"test\"\n",
        "\n",
        "def load_and_prepare_ragbench(repository: str, subsets: List[str], split: str) -> pd.DataFrame:\n",
        "    \"\"\"Loads specified RAGBench subsets and prepares them for the experiment loop.\"\"\"\n",
        "\n",
        "    target_subsets = subsets if isinstance(subsets, list) else [subsets]\n",
        "    print(f\"ðŸ“¡ Accessing RAGBench repository: {repository}...\")\n",
        "\n",
        "    datasets_list = []\n",
        "\n",
        "    for subset_name in target_subsets:\n",
        "        try:\n",
        "            print(f\"   -> Loading Subset: {subset_name} | Split: {split}\")\n",
        "            # Load the specific configuration (subset) from HuggingFace\n",
        "            ds = load_dataset(repository, name=subset_name, split=split)\n",
        "            df_subset = pd.DataFrame(ds)\n",
        "\n",
        "            # Essential for continuity: Tag the domain (e.g., Tech Support vs. Manuals)\n",
        "            df_subset['subset_source'] = subset_name\n",
        "\n",
        "            # RAGBench standardizes documents as a list of strings.\n",
        "            # We ensure this exists for our Retriever logic in Step 5.1\n",
        "            if 'documents' not in df_subset.columns and 'context' in df_subset.columns:\n",
        "                 df_subset.rename(columns={'context': 'documents'}, inplace=True)\n",
        "\n",
        "            datasets_list.append(df_subset)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"âš ï¸ Subset '{subset_name}' failed to load: {e}\")\n",
        "\n",
        "    if not datasets_list:\n",
        "        raise ValueError(\"âŒ No data loaded. Check repository name and subset keys.\")\n",
        "\n",
        "    # 1. Concatenate all subsets\n",
        "    df = pd.concat(datasets_list, ignore_index=True, sort=False)\n",
        "\n",
        "    # 2. Cleanup: Remove duplicates to avoid wasting LLM credits/time\n",
        "    initial_len = len(df)\n",
        "    df.drop_duplicates(subset=['question'], inplace=True)\n",
        "\n",
        "    # 3. Handle data types for downstream processing\n",
        "    # Ensure 'documents' is always a list of dicts/strings as expected by the Retriever\n",
        "    def clean_docs(docs):\n",
        "        if isinstance(docs, list): return docs\n",
        "        return [str(docs)] # Fallback if data is a single string\n",
        "\n",
        "    df['documents'] = df['documents'].apply(clean_docs)\n",
        "\n",
        "    print(f\"âœ… Data Ready! Loaded {len(df)} unique questions across {len(target_subsets)} domains.\")\n",
        "    if initial_len > len(df):\n",
        "        print(f\"âœ‚ï¸ Deduped {initial_len - len(df)} overlapping entries.\")\n",
        "\n",
        "    return df\n",
        "\n",
        "# Execute Loader\n",
        "df_ragbench = load_and_prepare_ragbench(DATASET_REPOSITORY, SUBSETS, SPLIT)\n",
        "\n",
        "# Preview for validation\n",
        "display(df_ragbench[['subset_source', 'question', 'documents']].head(3))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 330
        },
        "id": "bdfb5137",
        "outputId": "54ae4eed-c9d2-4e5a-b243-3451f278b759"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ðŸ“¡ Loading RAGBench: galileo-ai/ragbench | Split: test...\n",
            "   -> Loading subset: delucionqa...\n",
            "   -> Loading subset: emanual...\n",
            "   -> Loading subset: techqa...\n",
            "âœ… Dataset ready! Shape: (315, 27)\n",
            "ðŸ“Š Columns: ['id', 'question', 'documents', 'response', 'generation_model_name', 'annotating_model_name', 'dataset_name', 'documents_sentences', 'response_sentences', 'sentence_support_information', 'unsupported_response_sentence_keys', 'adherence_score', 'overall_supported_explanation', 'relevance_explanation', 'all_relevant_sentence_keys', 'all_utilized_sentence_keys', 'trulens_groundedness', 'trulens_context_relevance', 'ragas_faithfulness', 'ragas_context_relevance', 'gpt3_adherence', 'gpt3_context_relevance', 'gpt35_utilization', 'relevance_score', 'utilization_score', 'completeness_score', 'subset_source']\n"
          ]
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "summary": "{\n  \"name\": \"display(df[['subset_source', 'question', 'documents']]\",\n  \"rows\": 5,\n  \"fields\": [\n    {\n      \"column\": \"subset_source\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"delucionqa\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"question\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"What kind of safety features are implemented in this car?\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"documents\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}",
              "type": "dataframe"
            },
            "text/html": [
              "\n",
              "  <div id=\"df-e0174c4d-e48b-4cfb-8bd8-056da29ff1b2\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>subset_source</th>\n",
              "      <th>question</th>\n",
              "      <th>documents</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>delucionqa</td>\n",
              "      <td>What if I fail to latch the tailgate properly?</td>\n",
              "      <td>[{'text': ' Closing To close the tailgate, lif...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>delucionqa</td>\n",
              "      <td>What kind of safety features are implemented i...</td>\n",
              "      <td>[{'text': ' Some of the most important safety ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>delucionqa</td>\n",
              "      <td>When will the Automatic SOS be triggered?</td>\n",
              "      <td>[{'text': ' Automatic SOS â€” If Equipped Automa...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>delucionqa</td>\n",
              "      <td>What happens if I accidentally push the SOS Ca...</td>\n",
              "      <td>[{'text': ' Connected Services SOS FAQs â€” If E...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>delucionqa</td>\n",
              "      <td>What is the DEF?</td>\n",
              "      <td>[{'text': ' Adding Diesel Exhaust Fluid The DE...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-e0174c4d-e48b-4cfb-8bd8-056da29ff1b2')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-e0174c4d-e48b-4cfb-8bd8-056da29ff1b2 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-e0174c4d-e48b-4cfb-8bd8-056da29ff1b2');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "text/plain": [
              "  subset_source                                           question  \\\n",
              "0    delucionqa     What if I fail to latch the tailgate properly?   \n",
              "1    delucionqa  What kind of safety features are implemented i...   \n",
              "2    delucionqa          When will the Automatic SOS be triggered?   \n",
              "3    delucionqa  What happens if I accidentally push the SOS Ca...   \n",
              "4    delucionqa                                   What is the DEF?   \n",
              "\n",
              "                                           documents  \n",
              "0  [{'text': ' Closing To close the tailgate, lif...  \n",
              "1  [{'text': ' Some of the most important safety ...  \n",
              "2  [{'text': ' Automatic SOS â€” If Equipped Automa...  \n",
              "3  [{'text': ' Connected Services SOS FAQs â€” If E...  \n",
              "4  [{'text': ' Adding Diesel Exhaust Fluid The DE...  "
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from datasets import load_dataset\n",
        "\n",
        "# Determine which subsets to load (Defaults to techqa if not set)\n",
        "subsets_to_process = SUBSETS if 'SUBSETS' in globals() else [\"techqa\"]\n",
        "split_to_load = SPLIT if 'SPLIT' in globals() else \"test\"\n",
        "\n",
        "print(f\"ðŸ“¡ Loading RAGBench: {DATASET_REPOSITORY} | Split: {split_to_load}...\")\n",
        "\n",
        "datasets_to_load = []\n",
        "for subset_name in subsets_to_process:\n",
        "    try:\n",
        "        print(f\"   -> Loading subset: {subset_name}...\")\n",
        "        ds = load_dataset(DATASET_REPOSITORY, name=subset_name, split=split_to_load)\n",
        "        temp_df = pd.DataFrame(ds)\n",
        "\n",
        "        # Continuity: Track source for domain-specific metric breakdown later\n",
        "        temp_df['subset_source'] = subset_name\n",
        "        datasets_to_load.append(temp_df)\n",
        "    except Exception as e:\n",
        "        print(f\"âš ï¸ Warning: Could not load {subset_name}: {e}\")\n",
        "\n",
        "if not datasets_to_load:\n",
        "    raise ValueError(\"âŒ No datasets were successfully loaded. Check your DATASET_REPOSITORY.\")\n",
        "\n",
        "# 1. Master Concatenation\n",
        "df = pd.concat(datasets_to_load, ignore_index=True)\n",
        "\n",
        "# 2. Duplicate Removal (Crucial for multi-domain overlap)\n",
        "initial_len = len(df)\n",
        "df.drop_duplicates(subset=['question'], inplace=True, ignore_index=True)\n",
        "\n",
        "# 3. Document Normalization (Continuity with Chunker Step 5.1)\n",
        "# RAGBench docs come as a list of strings. We ensure the Chunker gets a clean list of Dicts\n",
        "def normalize_documents(docs):\n",
        "    if isinstance(docs, list):\n",
        "        # Convert list of strings to our pipeline's expected format: List[Dict]\n",
        "        return [{\"text\": str(d)} for d in docs]\n",
        "    elif isinstance(docs, str):\n",
        "        return [{\"text\": docs}]\n",
        "    return []\n",
        "\n",
        "df['documents'] = df['documents'].apply(normalize_documents)\n",
        "\n",
        "print(f\"âœ… Dataset ready! Shape: {df.shape}\")\n",
        "print(f\"ðŸ“Š Columns: {df.columns.tolist()}\")\n",
        "\n",
        "# Preview the data structure\n",
        "display(df[['subset_source', 'question', 'documents']].head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TeayxcwZt8yL"
      },
      "source": [
        "## 6. Extract All Documents and Build Retriever\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b1a71bce",
        "outputId": "afd3e5d0-8dd9-44ad-8abd-f4f6f39c24c5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ðŸ” Extracting unique technical documents from the dataset...\n",
            "âœ… Extraction Complete!\n",
            "ðŸ“Š Total unique documents to be indexed: 1105\n",
            "ðŸ“‰ Reduction Ratio: 0.29x\n"
          ]
        }
      ],
      "source": [
        "print(\"ðŸ” Extracting unique technical documents from the dataset...\")\n",
        "\n",
        "all_documents = []\n",
        "seen_docs = set()\n",
        "\n",
        "# RAGBench normalized format: List[Dict] from Step 6.1\n",
        "for doc_list in df['documents']:\n",
        "    for doc_entry in doc_list:\n",
        "        doc_text = doc_entry[\"text\"].strip()\n",
        "        if doc_text and doc_text not in seen_docs:\n",
        "            all_documents.append(doc_text)\n",
        "            seen_docs.add(doc_text)\n",
        "\n",
        "print(f\"âœ… Extraction Complete!\")\n",
        "print(f\"ðŸ“Š Total unique documents to be indexed: {len(all_documents)}\")\n",
        "print(f\"ðŸ“‰ Reduction Ratio: {len(df) / len(all_documents):.2f}x\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GQKxd9MXmq4c"
      },
      "outputs": [],
      "source": [
        "### Step 7: Text Processing and Indexing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/",
          "height": 293,
          "referenced_widgets": [
            "7fb24a4342874339b72c5748a637d671",
            "53cd2bd4d1e64099918c49493928f3f3",
            "c12b72a5e5ee4564bac4d1154778ee8f",
            "a9241050c79749a2909834738d00587c",
            "b79d235862204f1b997c68c592dd2b63",
            "e6227b6f85ca4f96951a0623f5069409",
            "2eec77a59c8c4d18a681aa8e1c0e2396",
            "1e47feb4c4bc4cf4b554c94c869d1a50",
            "6382b157b7a44797bc3bf5b7a1afedfe",
            "b97972b6433e4f368358d36bef0a4b03",
            "df7903f6dd654321849632e153f9acd9",
            "a63b7428989546ad8d04ed95c4830fcc",
            "4c6608e842e7428ca2af0c540d73518a",
            "0136f9517efd4c80980c053e97b28190",
            "60fbb183bed046de99391b62e82fabb8",
            "49544ca06fb54f648cd24251297fd1ea",
            "25e0d4e5af434ce58b5c538af2c2cbba",
            "e7d3d23035044caeaba4dfc853d1e15d",
            "3a578e10e3be49aca79e06ae23bc39fb"
          ]
        },
        "id": "ukUA2PByt8yM",
        "outputId": "a9a26e3e-c393-4247-f30b-371b55e5413b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ðŸš€ Loading models on cpu...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7fb24a4342874339b72c5748a637d671",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "53cd2bd4d1e64099918c49493928f3f3",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config_sentence_transformers.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c12b72a5e5ee4564bac4d1154778ee8f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "README.md: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a9241050c79749a2909834738d00587c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "sentence_bert_config.json:   0%|          | 0.00/52.0 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b79d235862204f1b997c68c592dd2b63",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/779 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e6227b6f85ca4f96951a0623f5069409",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/1.34G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2eec77a59c8c4d18a681aa8e1c0e2396",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/366 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1e47feb4c4bc4cf4b554c94c869d1a50",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "vocab.txt: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6382b157b7a44797bc3bf5b7a1afedfe",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.json: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b97972b6433e4f368358d36bef0a4b03",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/125 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "df7903f6dd654321849632e153f9acd9",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/191 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a63b7428989546ad8d04ed95c4830fcc",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/795 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4c6608e842e7428ca2af0c540d73518a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/2.27G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0136f9517efd4c80980c053e97b28190",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "60fbb183bed046de99391b62e82fabb8",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "sentencepiece.bpe.model:   0%|          | 0.00/5.07M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "49544ca06fb54f648cd24251297fd1ea",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/17.1M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "25e0d4e5af434ce58b5c538af2c2cbba",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/964 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e7d3d23035044caeaba4dfc853d1e15d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "README.md: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ‚ï¸ Chunking and Embedding...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3a578e10e3be49aca79e06ae23bc39fb",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Batches:   0%|          | 0/45 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ðŸ Vector store complete. Total entries: 1439\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import nltk\n",
        "from sentence_transformers import SentenceTransformer, CrossEncoder\n",
        "\n",
        "DEFAULT_EXPERIMENT = EXPERIMENTS[0] # Use the first experiment as a default configuration\n",
        "\n",
        "CHUNKING_STRATEGY = DEFAULT_EXPERIMENT.get(\"chunking_strategy\", \"sentence_based\")\n",
        "RETRIEVAL_TYPE = DEFAULT_EXPERIMENT.get(\"retrieval_type\", \"hybrid\")\n",
        "USE_CROSS_ENCODER = DEFAULT_EXPERIMENT.get(\"use_cross_encoder\", True)\n",
        "\n",
        "# CHUNK_SIZE and CHUNK_OVERLAP are not directly in EXPERIMENTS config, using DocumentChunker defaults\n",
        "CHUNK_SIZE = 500\n",
        "CHUNK_OVERLAP = 50\n",
        "\n",
        "# 1. Environment Check\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "# 2. Models & Components\n",
        "print(f\"ðŸš€ Loading models on {device}...\")\n",
        "embedding_model = SentenceTransformer(EMBEDDING_MODEL, device=device)\n",
        "EMBEDDING_DIM = embedding_model.get_sentence_embedding_dimension()\n",
        "\n",
        "cross_encoder = None\n",
        "if USE_CROSS_ENCODER:\n",
        "    cross_encoder = CrossEncoder(CROSS_ENCODER_MODEL, device=device)\n",
        "\n",
        "# Initialize Chunker & Store\n",
        "chunker = DocumentChunker(CHUNKING_STRATEGY, {\n",
        "    \"chunk_size\": CHUNK_SIZE,\n",
        "    \"chunk_overlap\": CHUNK_OVERLAP\n",
        "})\n",
        "vector_store = FAISSVectorStore(EMBEDDING_DIM, \"InnerProduct\")\n",
        "\n",
        "# 3. Processing & Indexing\n",
        "print(\"âœ‚ï¸ Chunking and Embedding...\")\n",
        "chunks_metadata = chunker.chunk_documents(all_documents)\n",
        "chunk_texts = [c['text'] for c in chunks_metadata]\n",
        "\n",
        "# Generate dense vectors\n",
        "embeddings = embedding_model.encode(chunk_texts, show_progress_bar=True, batch_size=32)\n",
        "\n",
        "# 4. Finalize Knowledge Base\n",
        "vector_store.add_documents(embeddings.astype('float32'), chunks_metadata)\n",
        "print(f\"ðŸ Vector store complete. Total entries: {vector_store.index.ntotal}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "60db9575"
      },
      "outputs": [],
      "source": [
        "from rank_bm25 import BM25Okapi\n",
        "\n",
        "class DenseRetriever:\n",
        "    def __init__(self, vector_store, embedding_model):\n",
        "        self.vector_store = vector_store\n",
        "        self.embedding_model = embedding_model\n",
        "\n",
        "    def retrieve(self, query: str, top_k: int) -> List[Dict]:\n",
        "        q_emb = self.embedding_model.encode([query]).astype('float32')\n",
        "        # Return full metadata dicts for the Judge\n",
        "        metadata = self.vector_store.search(q_emb, top_k) # Corrected from unpacking 3 values\n",
        "        return metadata\n",
        "\n",
        "class SparseRetriever:\n",
        "    def __init__(self, chunks: List[Dict]):\n",
        "        self.chunks = chunks\n",
        "        tokenized_corpus = [c['text'].lower().split() for c in chunks]\n",
        "        self.bm25 = BM25Okapi(tokenized_corpus)\n",
        "\n",
        "    def retrieve(self, query: str, top_k: int) -> List[Dict]:\n",
        "        q_tokens = query.lower().split()\n",
        "        scores = self.bm25.get_scores(q_tokens)\n",
        "        top_idx = np.argsort(scores)[::-1][:top_k]\n",
        "        return [self.chunks[i] for i in top_idx]\n",
        "\n",
        "class HybridRetrieverWithRerank:\n",
        "    def __init__(self, vector_store, embedding_model, cross_encoder, chunks):\n",
        "        self.dense = DenseRetriever(vector_store, embedding_model)\n",
        "        self.sparse = SparseRetriever(chunks)\n",
        "        self.cross_encoder = cross_encoder\n",
        "\n",
        "    def retrieve(self, query: str, top_k: int) -> List[Dict]:\n",
        "        # 1. Broad retrieval pool\n",
        "        d_res = self.dense.retrieve(query, top_k * 2)\n",
        "        s_res = self.sparse.retrieve(query, top_k * 2)\n",
        "\n",
        "        # Deduplicate based on text\n",
        "        candidates = {c['text']: c for c in (d_res + s_res)}.values()\n",
        "        candidate_list = list(candidates)\n",
        "\n",
        "        # 2. Cross-Encoder Rerank\n",
        "        pairs = [[query, c['text']] for c in candidate_list]\n",
        "        scores = self.cross_encoder.predict(pairs)\n",
        "\n",
        "        # 3. Sort & Slice\n",
        "        ranked = sorted(zip(candidate_list, scores), key=lambda x: x[1], reverse=True)\n",
        "        return [res[0] for res in ranked[:top_k]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "fZRZyr_Xt8yM",
        "outputId": "ab6e09a7-43ca-42d6-ac97-dbe187bb0d4c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âš™ï¸ Building hybrid retriever...\n",
            "âœ… Pipeline Configured. Ready for Experiment Run.\n"
          ]
        }
      ],
      "source": [
        "# 7.3: Dynamic Initialization\n",
        "\n",
        "# 7.3 Unified Retriever Initialization\n",
        "print(f\"âš™ï¸ Building {RETRIEVAL_TYPE} retriever...\")\n",
        "\n",
        "if RETRIEVAL_TYPE == \"dense\":\n",
        "    retriever = DenseRetriever(vector_store, embedding_model)\n",
        "elif RETRIEVAL_TYPE == \"sparse\":\n",
        "    retriever = SparseRetriever(chunks_metadata)\n",
        "elif RETRIEVAL_TYPE == \"hybrid\":\n",
        "    retriever = HybridRetrieverWithRerank(\n",
        "        vector_store, embedding_model, cross_encoder, chunks_metadata\n",
        "    )\n",
        "\n",
        "print(f\"âœ… Pipeline Configured. Ready for Experiment Run.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AqpGWBvBbW2p"
      },
      "source": [
        "### 7.1 Multi-Experiment Runner (Run this if RUN_MULTI_EXPERIMENTS = True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/",
          "height": 136,
          "referenced_widgets": [
            "c6a6132f588c4c0eb6b21529a986b770"
          ]
        },
        "id": "xxN7GXlybW2p",
        "outputId": "021227fe-5f83-42d2-bc2c-2a125cd2c265"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ðŸš€ Starting Benchmark on 5 Configurations...\n",
            "\n",
            "â–¶ï¸ Running: cs_sentence_hybrid_delucionqa (Subset: delucionqa)\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c6a6132f588c4c0eb6b21529a986b770",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Benchmarking:   0%|          | 0/5 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "â–¶ï¸ Running: cs_fixed_bm25_techqa (Subset: techqa)\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tqdm.auto import tqdm\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "RUN_MULTI_EXPERIMENTS = True # Define the flag to enable multi-experiment runs\n",
        "\n",
        "# 1. UPDATED HYBRID RETRIEVER (With Reciprocal Rank Fusion / Alpha Logic)\n",
        "class HybridRetriever:\n",
        "    def __init__(self, dense, sparse, alpha=0.5):\n",
        "        self.dense = dense\n",
        "        self.sparse = sparse\n",
        "        self.alpha = alpha # 0.0 = Pure Sparse, 1.0 = Pure Dense\n",
        "\n",
        "    def retrieve(self, query: str, top_k: int) -> List[Dict]:\n",
        "        dense_hits = self.dense.retrieve(query, top_k * 2)\n",
        "        sparse_hits = self.sparse.retrieve(query, top_k * 2)\n",
        "\n",
        "        # Merge using alpha-weighted ranking or simple union\n",
        "        combined = {}\n",
        "        # Simple union for metadata preservation; in production, use RRF scores\n",
        "        for i, hit in enumerate(dense_hits):\n",
        "            combined[hit[\"text\"]] = hit # Deduplicate by text\n",
        "        for i, hit in enumerate(sparse_hits):\n",
        "            if hit[\"text\"] not in combined:\n",
        "                combined[hit[\"text\"]] = hit\n",
        "\n",
        "        return list(combined.values())[:top_k]\n",
        "\n",
        "# 2. THE MAIN RUNNER\n",
        "if RUN_MULTI_EXPERIMENTS:\n",
        "    all_experiment_summary = []\n",
        "\n",
        "    print(f\"ðŸš€ Starting Benchmark on {len(EXPERIMENTS)} Configurations...\")\n",
        "\n",
        "    for exp in EXPERIMENTS:\n",
        "        print(f\"\\nâ–¶ï¸ Running: {exp['name']} (Subset: {exp['subset']})\")\n",
        "\n",
        "        # A. Setup Experiment-Specific Pipeline\n",
        "        # Load the specific embedding model defined in Step 3 Config\n",
        "        model = SentenceTransformer(exp['embedding_model'], device=device)\n",
        "\n",
        "        # Initialize the specific chunker\n",
        "        chunk_cfg = {\"chunk_size\": CHUNK_SIZE, \"chunk_overlap\": CHUNK_OVERLAP}\n",
        "        chunker = DocumentChunker(exp['chunking_strategy'], chunk_cfg)\n",
        "\n",
        "        # B. Process Knowledge Base (Unique to this experiment's strategy)\n",
        "        # We reuse the df_ragbench loaded in Step 6.1 filtered for this subset\n",
        "        exp_df = df[df['subset_source'] == exp['subset']].copy() # Changed from df_ragbench to df\n",
        "\n",
        "        # Flatten and index\n",
        "        unique_texts = list(set([d['text'] for docs in exp_df['documents'] for d in docs]))\n",
        "        chunks_metadata = chunker.chunk_documents(unique_texts)\n",
        "\n",
        "        # Index in FAISS\n",
        "        vectors = model.encode([c['text'] for c in chunks_metadata], show_progress_bar=False)\n",
        "        vs = FAISSVectorStore(model.get_sentence_embedding_dimension(), \"InnerProduct\")\n",
        "        vs.add_documents(vectors.astype('float32'), chunks_metadata)\n",
        "\n",
        "        # C. Initialize Retriever based on Type\n",
        "        d_ret = DenseRetriever(vs, model)\n",
        "        if exp['retrieval_type'] == \"hybrid\":\n",
        "            s_ret = SparseRetriever(chunks_metadata)\n",
        "            active_retriever = HybridRetriever(d_ret, s_ret, exp.get('alpha', 0.5))\n",
        "        else:\n",
        "            active_retriever = d_ret\n",
        "\n",
        "        # D. Evaluation Loop\n",
        "        results_storage = []\n",
        "        # Evaluation sample size (N=5 for testing, N=50+ for production)\n",
        "        eval_size = min(5, len(exp_df))\n",
        "\n",
        "        for _, row in tqdm(exp_df.head(eval_size).iterrows(), total=eval_size, desc=\"Benchmarking\"):\n",
        "            query = row['question']\n",
        "\n",
        "            # 1. Retrieve\n",
        "            retrieved = active_retriever.retrieve(query, exp['top_k_final'])\n",
        "\n",
        "            # 2. Generate (Continuity with Step 5.6)\n",
        "            response = generate_response(query, retrieved)\n",
        "\n",
        "            # 3. Judge (LLM-as-a-judge metrics)\n",
        "            attr = extract_attributes(query, retrieved, response)\n",
        "            metrics = compute_all_metrics(attr, retrieved)\n",
        "\n",
        "            results_storage.append(metrics)\n",
        "\n",
        "        # E. Aggregate Results\n",
        "        avg_metrics = pd.DataFrame(results_storage).mean().to_dict()\n",
        "        all_experiment_summary.append({\n",
        "            \"Experiment\": exp['name'],\n",
        "            \"Subset\": exp['subset'],\n",
        "            \"Strategy\": exp['chunking_strategy'],\n",
        "            **avg_metrics\n",
        "        })\n",
        "\n",
        "    # 3. FINAL LEADERBOARD\n",
        "    leaderboard = pd.DataFrame(all_experiment_summary)\n",
        "    display(leaderboard.sort_values(by=\"faithfulness_score\", ascending=False))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tXmtxLIxbW2q"
      },
      "source": [
        "## 9. Save Results\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "66puu8iCbW2q"
      },
      "outputs": [],
      "source": [
        "if SAVE_RESULTS:\n",
        "    import os\n",
        "    os.makedirs(RESULTS_DIR, exist_ok=True)\n",
        "\n",
        "    # Save per-sample results\n",
        "    per_sample_data = []\n",
        "    for r in results['per_sample']:\n",
        "        per_sample_data.append({\n",
        "            'id': r['id'],\n",
        "            'question': r['question'],\n",
        "            'context_relevance': r['metrics']['context_relevance'],\n",
        "            'context_utilization': r['metrics']['context_utilization'],\n",
        "            'completeness': r['metrics']['completeness'],\n",
        "            'adherence': r['metrics']['adherence'],\n",
        "            'response': r['response']\n",
        "        })\n",
        "\n",
        "    per_sample_df = pd.DataFrame(per_sample_data)\n",
        "    per_sample_df.to_csv(f\"{RESULTS_DIR}/per_sample_results.csv\", index=False)\n",
        "    print(f\"âœ… Saved per-sample results to {RESULTS_DIR}/per_sample_results.csv\")\n",
        "\n",
        "    # Save comparison results\n",
        "    comparison_df = pd.DataFrame(comparison_results).T\n",
        "    comparison_df.to_csv(f\"{RESULTS_DIR}/comparison_results.csv\")\n",
        "    print(f\"âœ… Saved comparison results to {RESULTS_DIR}/comparison_results.csv\")\n",
        "\n",
        "    # Save full results as JSON (without full documents to save space)\n",
        "    results_summary = {\n",
        "        'config': {\n",
        "            'domain': DOMAIN,\n",
        "            'subset': SUBSET,\n",
        "            'chunking_strategy': CHUNKING_STRATEGY,\n",
        "            'retrieval_type': RETRIEVAL_TYPE,\n",
        "            'embedding_model': EMBEDDING_MODEL\n",
        "        },\n",
        "        'comparison': comparison_results,\n",
        "        'predicted_metrics_summary': {\n",
        "            metric: {\n",
        "                'mean': np.mean(scores),\n",
        "                'std': np.std(scores),\n",
        "                'min': np.min(scores),\n",
        "                'max': np.max(scores)\n",
        "            }\n",
        "            for metric, scores in results['predicted_metrics'].items()\n",
        "        }\n",
        "    }\n",
        "\n",
        "    with open(f\"{RESULTS_DIR}/results_summary.json\", 'w') as f:\n",
        "        json.dump(results_summary, f, indent=2)\n",
        "    print(f\"âœ… Saved results summary to {RESULTS_DIR}/results_summary.json\")\n",
        "\n",
        "    print(f\"\\nðŸ“Š All results saved to {RESULTS_DIR}/\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UT77wjHSbW2q"
      },
      "source": [
        "## 10. Visualization of Results\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "kXnOFCSobW2r"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Create visualization of metrics comparison\n",
        "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
        "axes = axes.flatten()\n",
        "\n",
        "metrics_list = ['context_relevance', 'context_utilization', 'completeness', 'adherence']\n",
        "\n",
        "for idx, metric in enumerate(metrics_list):\n",
        "    if metric in results['predicted_metrics'] and metric in ground_truth:\n",
        "        pred = results['predicted_metrics'][metric]\n",
        "        gt = ground_truth[metric]\n",
        "\n",
        "        min_len = min(len(pred), len(gt))\n",
        "        pred = pred[:min_len]\n",
        "        gt = gt[:min_len]\n",
        "\n",
        "        axes[idx].scatter(gt, pred, alpha=0.5)\n",
        "        axes[idx].plot([0, 1], [0, 1], 'r--', label='Perfect prediction')\n",
        "        axes[idx].set_xlabel('Ground Truth')\n",
        "        axes[idx].set_ylabel('Predicted')\n",
        "        axes[idx].set_title(f'{metric.replace(\"_\", \" \").title()}')\n",
        "        axes[idx].legend()\n",
        "        axes[idx].grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(f\"{RESULTS_DIR}/metrics_comparison.png\", dpi=150, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "print(\"âœ… Visualization saved!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "87GF7AYpbW2r"
      },
      "source": [
        "## 11. Summary and Next Steps\n",
        "\n",
        "### Summary\n",
        "- âœ… Dataset loaded and processed\n",
        "- âœ… Retriever built with all documents\n",
        "- âœ… Evaluation completed on all questions\n",
        "- âœ… Metrics computed and compared with ground truth\n",
        "- âœ… Results saved\n",
        "\n",
        "### Next Steps for Optimization\n",
        "1. **Experiment with different chunking strategies**: Try fixed_size, paragraph_based, sliding_window\n",
        "2. **Try domain-specific embedding models**: Use BioBERT for biomedical, Legal-BERT for legal, etc.\n",
        "3. **Experiment with retrieval types**: Try sparse (BM25) or hybrid search\n",
        "4. **Vary Top-K values**: Test with different numbers of retrieved documents\n",
        "5. **Try different LLM models**: Experiment with different generation models\n",
        "6. **Scale to other domains**: Once optimized for one domain, extend to others\n",
        "\n",
        "### Tips for Kaggle\n",
        "- Use Kaggle Secrets to store your Groq API key securely\n",
        "- Enable GPU if needed for faster embedding generation\n",
        "- Use Kaggle's persistent storage for saving results\n",
        "- Submit as a Kaggle Notebook for version control\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AT4Zce42bW2r"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "ymwBYaHCt8yM"
      },
      "outputs": [],
      "source": [
        "# Define helper retriever classes needed for the conditional logic\n",
        "class DenseRetriever:\n",
        "    def __init__(self, vector_store, embedding_model):\n",
        "        self.vector_store = vector_store\n",
        "        self.embedding_model = embedding_model\n",
        "\n",
        "    def retrieve(self, query: str, top_k: int) -> List[str]:\n",
        "        q_emb = self.embedding_model.encode([query])\n",
        "        dense_texts = self.vector_store.search(q_emb, top_k) # Corrected to unpack a single value\n",
        "        return dense_texts\n",
        "\n",
        "class SparseRetriever:\n",
        "    def __init__(self, all_documents_for_bm25: List[str]):\n",
        "        self.all_documents = all_documents_for_bm25\n",
        "        tokenized_corpus = [doc.lower().split() for doc in all_documents_for_bm25]\n",
        "        self.bm25 = BM25Okapi(tokenized_corpus)\n",
        "\n",
        "    def retrieve(self, query: str, top_k: int) -> List[str]:\n",
        "        q_tokens = query.lower().split()\n",
        "        bm25_scores = self.bm25.get_scores(q_tokens)\n",
        "        top_bm25_idx = np.argsort(bm25_scores)[::-1][:top_k]\n",
        "        bm25_texts = [self.all_documents[i] for i in top_bm25_idx]\n",
        "        return bm25_texts\n",
        "\n",
        "class HybridRetriever:\n",
        "    def __init__(self, dense_retriever, sparse_retriever, hybrid_alpha: float):\n",
        "        self.dense_retriever = dense_retriever\n",
        "        self.sparse_retriever = sparse_retriever\n",
        "        self.hybrid_alpha = hybrid_alpha # Alpha for blending, though simple union for now\n",
        "\n",
        "    def retrieve(self, query: str, top_k: int) -> List[str]:\n",
        "        # Retrieve candidates from both dense and sparse\n",
        "        dense_results = self.dense_retriever.retrieve(query, top_k)\n",
        "        sparse_results = self.sparse_retriever.retrieve(query, top_k)\n",
        "\n",
        "        # Combine and ensure uniqueness, then return top_k\n",
        "        combined_results = list(set(dense_results + sparse_results))\n",
        "        return combined_results[:top_k]\n",
        "\n",
        "\n",
        "# Initialize retriever based on configuration\n",
        "if RETRIEVAL_TYPE == \"dense\":\n",
        "    retriever = DenseRetriever(vector_store, embedding_model)\n",
        "elif RETRIEVAL_TYPE == \"sparse\":\n",
        "    retriever = SparseRetriever(all_documents)\n",
        "elif RETRIEVAL_TYPE == \"hybrid\":\n",
        "    if USE_CROSS_ENCODER:\n",
        "        # Assuming HybridRetrieverWithRerank is the intended 'hybrid' with reranking\n",
        "        retriever = HybridRetrieverWithRerank(vector_store, embedding_model, cross_encoder, chunks_metadata)\n",
        "    else:\n",
        "        # If no cross-encoder, use the simpler HybridRetriever defined above\n",
        "        dense_ret = DenseRetriever(vector_store, embedding_model)\n",
        "        sparse_ret = SparseRetriever(all_documents)\n",
        "        retriever = HybridRetriever(dense_ret, sparse_ret, HYBRID_ALPHA)\n",
        "else:\n",
        "    raise ValueError(f\"Unknown retrieval type: {RETRIEVAL_TYPE}\")\n",
        "\n",
        "# Ensure the corpus is set for BM25 in case of sparse or hybrid retrieval that uses it\n",
        "# HybridRetrieverWithRerank needs set_corpus called after all_documents is available\n",
        "if hasattr(retriever, 'set_corpus') and retriever.all_texts == []: # Only set if not already set\n",
        "    retriever.set_corpus(all_documents)\n",
        "\n",
        "print(f\"âœ… Retriever initialized: {RETRIEVAL_TYPE}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "teMG1xOLt8yM"
      },
      "source": [
        "## 7. Run Evaluation Pipeline\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "THdvlsopt8yM"
      },
      "outputs": [],
      "source": [
        "ground_truth = {}\n",
        "metrics = ['relevance_score', 'utilization_score', 'completeness_score', 'adherence_score']\n",
        "for metric in metrics:\n",
        "    if metric in df.columns:\n",
        "        ground_truth[metric] = df[metric].tolist()\n",
        "    else:\n",
        "        print(f\"âš ï¸ Warning: {metric} not found in dataset\")\n",
        "        ground_truth[metric] = []\n",
        "\n",
        "print(\"Ground truth scores available:\")\n",
        "for metric, scores in ground_truth.items():\n",
        "    if scores:\n",
        "        print(f\"  {metric}: {len(scores)} samples, mean={np.mean(scores):.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "yKHdGFXEt8yM"
      },
      "outputs": [],
      "source": [
        "# Initialize results storage\n",
        "results = {\n",
        "    'per_sample': [],\n",
        "    'predicted_metrics': {\n",
        "        'relevance_score': [],\n",
        "        'utilization_score': [],\n",
        "        'completeness_score': [],\n",
        "        'adherence_score': []\n",
        "    }\n",
        "}\n",
        "\n",
        "print(f\"Starting evaluation on {len(df)} samples...\")\n",
        "print(\"This may take a while depending on dataset size and API rate limits...\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XTsnnxf8t8yO"
      },
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [],
      "dockerImageVersionId": 31192,
      "isGpuEnabled": false,
      "isInternetEnabled": false,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}